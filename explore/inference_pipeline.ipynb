{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69c577e-b7ae-4a88-a91e-d689002cec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2024-09-23 09:11:19.636891: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 09:11:19.869471: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/nvtabular/loader/__init__.py:19: DeprecationWarning: The `nvtabular.loader` module has moved to a new repository, at https://github.com/NVIDIA-Merlin/dataloader .  Support for importing from `nvtabular.loader` is deprecated, and will be removed in a future version. Please update your imports to refer to `merlinloader`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['NUMBA_DISABLE_JIT'] = \"1\"\n",
    "os.environ[\"NUMBA_DISABLE_CUDA\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "import seedir as sd\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9725b1-b40d-4197-9931-82e4a741a284",
   "metadata": {},
   "source": [
    "## Register our features on feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee1785-ff04-4e83-a019-671616839713",
   "metadata": {},
   "source": [
    "The Feast feature registry is a central catalog of all the feature definitions and their related metadata(read more here). We have defined our user and item features definitions in the user_features.py and `item_features.py` files. With `FeatureView()` users can register data sources in their organizations into Feast, and then use those data sources for both training and online inference. In the `user_features.py` and `item_features.py` files, we are telling `Feast` where to find user and item features. \\\r\n",
    "https://docs.feast.dev/getting-started/architecture-and-components/registry\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161b9f1-64c5-4d08-98ff-5a190999b451",
   "metadata": {},
   "source": [
    "Before we move on to the next steps, we need to perform `feast apply` command as directed below. With that, we register our features, we can apply the changes to create our `feature registry` and `store` all entity and feature view definitions in a local `SQLite` online store called `online_store.db`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81f9ca8-8342-4b2f-a73f-c957f5395942",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"/try-merlin/\")\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/try-merlin/data/\")\n",
    "\n",
    "# define feature repo path\n",
    "feast_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6891b51-5ecf-4470-aa17-b0be196cc76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/try-merlin/feast_repo/feature_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "Created entity \u001b[1m\u001b[32muser_id\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mitem_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32muser_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mitem_features\u001b[0m\n",
      "\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "09/23/2024 09:12:07 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_item_features\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_user_features\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd $feast_repo_path\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04107e62-ff37-4404-9954-d86d5cd172dc",
   "metadata": {},
   "source": [
    "## Loading features from offline store into an online store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce01d04-ac03-4d67-b394-ff054a4b5998",
   "metadata": {},
   "source": [
    "After we execute apply and registered our features and created our online local store, now we need to perform `materialization` operation. This is done to keep our online store up to date and get it ready for prediction. For that we need to run a job that loads feature data from our feature view sources into our online store. As we add new features to our offline stores, we can continuously materialize them to keep our online store up to date by finding the latest feature values for each user.\n",
    "\n",
    "https://docs.feast.dev/how-to-guides/running-feast-in-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5707fba5-b183-4993-94b5-d334ba49bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/try-merlin/feast_repo/feature_repo\n",
      "09/23/2024 09:12:15 AM root WARNING: _list_feature_views will make breaking changes. Please use _list_batch_feature_views instead. _list_feature_views will behave like _list_all_feature_views in the future.\n",
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m1995-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m:\n",
      "  0%|                                                                       | 0/446 [00:00<?, ?it/s]09/23/2024 09:12:15 AM root WARNING: Cannot use sqlite_vec for vector search\n",
      "100%|███████████████████████████████████████████████████████████| 446/446 [00:00<00:00, 3079.47it/s]\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m:\n",
      "100%|███████████████████████████████████████████████████████████| 453/453 [00:00<00:00, 8700.76it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd $feast_repo_path\n",
    "!feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf99ad6-e7b0-4461-912c-a032598e5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feast_repo/\n",
      "├─README.md\n",
      "├─__init__.py\n",
      "└─feature_repo/\n",
      "  ├─__init__.py\n",
      "  ├─data/\n",
      "  │ ├─item_features.parquet\n",
      "  │ ├─online_store.db\n",
      "  │ ├─registry.db\n",
      "  │ └─user_features.parquet\n",
      "  ├─feature_store.yaml\n",
      "  ├─item_features.py\n",
      "  ├─test_workflow.py\n",
      "  └─user_features.py\n"
     ]
    }
   ],
   "source": [
    "# set up the base dir to for feature store\n",
    "sd.seedir(os.path.join(BASE_DIR, 'feast_repo'), style='lines', itemlimit=10, depthlimit=5, exclude_folders=['.ipynb_checkpoints', '__pycache__'], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2079c6-b1cf-487d-82fc-258fbcc18cec",
   "metadata": {},
   "source": [
    "## Set up Faiss index, create feature store client and objects for the Triton ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828fc6a5-a7fa-45e8-93f0-ea5d6009c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR, 'faiss_index')):\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'faiss_index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471b67e8-10fb-49b6-8a28-aea19f4b4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = os.path.join(BASE_DIR, 'faiss_index', \"index.faiss\")\n",
    "retrieval_model_path = os.path.join(BASE_DIR, \"query_tower/\")\n",
    "ranking_model_path = os.path.join(BASE_DIR, \"dlrm/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9f276-88e9-434a-a397-329e2785c4e1",
   "metadata": {},
   "source": [
    "`QueryFaiss` operator creates an interface between a FAISS Approximate Nearest Neighbors (ANN) Index and Triton Inference Server. For a given input query vector, we do an ANN search query to find the ids of top-k nearby nodes in the index. \n",
    "\n",
    "`setup_faiss` is a utility function that will create a Faiss index from an embedding vector with using L2 distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bc1151-5094-4ec8-af2f-fb5a0e2037b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 453 points to 32 centroids: please provide at least 1248 training points\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss \n",
    "\n",
    "item_embeddings = pd.read_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))\n",
    "setup_faiss(item_embeddings, faiss_index_path, embedding_column=\"output_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ea4bcc-7358-4f1a-b791-67f87d6b93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(feast_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80117555-2901-4cb0-b79c-5a604c45eff8",
   "metadata": {},
   "source": [
    "Fetch user features with `QueryFeast` operator from the feature store. `QueryFeast` operator is responsible for ensuring that our feast feature store can communicate correctly with `tritonserver` for the ensemble feast feature look ups.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a11753-dab2-4e63-8ab5-c6cde828d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2024-09-23 09:12:26+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m from \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2024-09-23 09:12:26+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "user_attributes = [\"user_id\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94904db-dd81-4263-8bf4-6c4ce0bbed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular import Workflow\n",
    "\n",
    "nvt_workflow = Workflow.load(os.path.join(DATA_FOLDER, 'processed_nvt/workflow'))\n",
    "user_subgraph = nvt_workflow.get_subworkflow(\"user\")\n",
    "user_features = user_attributes >> TransformWorkflow(user_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f188e8-84ef-49e3-b4f1-f3fb59f11e54",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9336bd-8525-415e-8ad3-c81af344bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevent TF to claim all GPU memory\n",
    "from merlin.dataloader.tf_utils import configure_tensorflow\n",
    "\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31873a75-f975-4dfa-994e-6ea0147a51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpz7hstpie/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpz7hstpie/assets\n"
     ]
    }
   ],
   "source": [
    "topk_retrieval = int(\n",
    "    os.environ.get(\"topk_retrieval\", \"100\")\n",
    ")\n",
    "retrieval = (\n",
    "    user_features\n",
    "    >> PredictTensorflow(retrieval_model_path)\n",
    "    >> QueryFaiss(faiss_index_path, topk=topk_retrieval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108ca9ca-63bd-4a06-b593-c7bfb834eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2024-09-23 09:12:34+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m from \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2024-09-23 09:12:34+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "item_attributes = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41c744fd-8d24-4544-a463-3343a72744f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_subgraph = nvt_workflow.get_subworkflow(\"item\")\n",
    "item_features = item_attributes >> TransformWorkflow(item_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd148-975d-4e3f-9f11-099c5fc58799",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the “unroll” columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4161ff89-128c-4aa1-9d25-86700d34f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef6ba1-ebe4-4622-896e-d8b3b6b00443",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to PredictTensorflow() operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b570dc-0560-4075-9341-b737a5af1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 98). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyslm4f7x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyslm4f7x/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909e4d3-43e5-48d0-96ce-f88bb1ea2a61",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d6a776-72a8-4e21-8fab-8dd82c3cdce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=10\n",
    "ordering = combined_features[\"item_id\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"click/binary_classification_task\"], topk=top_k, temperature=0.00000001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0b281-0cea-4f66-a1b6-cd3e26730105",
   "metadata": {},
   "source": [
    "## Export Graph as Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e638c30-80a9-430f-8861-898c9a075f0c",
   "metadata": {},
   "source": [
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph. After we create the ensemble object, we export the graph, supplying an export path for the ensemble.export() function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs.\n",
    "\n",
    "Create the folder to export the models and config files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf622db0-1f42-493b-b603-72344e0089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(BASE_DIR, 'poc_ensemble')):\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'poc_ensemble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93beb6c8-53ca-4989-b247-56c0ecf0e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50ea7972-6ddb-4039-8b22-7883e340961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered_ids', 'ordered_scores']\n"
     ]
    }
   ],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join(BASE_DIR, 'poc_ensemble')\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)\n",
    "\n",
    "# return the output column name\n",
    "outputs = ensemble.graph.output_schema.column_names\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80445019-ed40-482c-99e6-f39b2b67b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poc_ensemble/\n",
      "├─0_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.user_age.parquet\n",
      "│ │   │ ├─unique.user_brands.parquet\n",
      "│ │   │ ├─unique.user_categories.parquet\n",
      "│ │   │ ├─unique.user_consumption_2.parquet\n",
      "│ │   │ ├─unique.user_gender.parquet\n",
      "│ │   │ ├─unique.user_geography.parquet\n",
      "│ │   │ ├─unique.user_group.parquet\n",
      "│ │   │ ├─unique.user_id.parquet\n",
      "│ │   │ ├─unique.user_intentions.parquet\n",
      "│ │   │ └─unique.user_is_occupied.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─1_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.item_brand.parquet\n",
      "│ │   │ ├─unique.item_category.parquet\n",
      "│ │   │ ├─unique.item_id.parquet\n",
      "│ │   │ └─unique.item_shop.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─3_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ ├─input_schema.json\n",
      "│ │   │ └─output_schema.json\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "└─executor_model/\n",
      "  ├─1/\n",
      "  │ ├─ensemble/\n",
      "  │ │ ├─ensemble.pkl\n",
      "  │ │ ├─index.faiss\n",
      "  │ │ └─metadata.json\n",
      "  │ └─model.py\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "sd.seedir(export_path, style='lines', itemlimit=10, depthlimit=5, exclude_folders=['.ipynb_checkpoints', '__pycache__'], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1220f86-a5d4-43f9-abfe-461da7f24463",
   "metadata": {},
   "source": [
    "## Starting Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499f8ea4-ec83-4dda-b783-6883663c040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0922 05:19:29.380894 388 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\n",
      "I0922 05:19:29.381689 388 cuda_memory_manager.cc:115] CUDA memory pool disabled\n",
      "I0922 05:19:29.620446 388 model_lifecycle.cc:462] loading: 3_predicttensorflowtriton:1\n",
      "I0922 05:19:29.623060 388 model_lifecycle.cc:462] loading: 2_transformworkflowtriton:1\n",
      "I0922 05:19:29.625543 388 model_lifecycle.cc:462] loading: 1_predicttensorflowtriton:1\n",
      "I0922 05:19:29.628507 388 model_lifecycle.cc:462] loading: executor_model:1\n",
      "I0922 05:19:29.631335 388 model_lifecycle.cc:462] loading: 0_transformworkflowtriton:1\n",
      "I0922 05:19:29.969806 388 tensorflow.cc:2577] TRITONBACKEND_Initialize: tensorflow\n",
      "I0922 05:19:29.969854 388 tensorflow.cc:2587] Triton TRITONBACKEND API version: 1.13\n",
      "I0922 05:19:29.969857 388 tensorflow.cc:2593] 'tensorflow' TRITONBACKEND API version: 1.13\n",
      "I0922 05:19:29.969860 388 tensorflow.cc:2617] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"version\":\"2\",\"default-max-batch-size\":\"4\"}}\n",
      "I0922 05:19:29.970275 388 tensorflow.cc:2683] TRITONBACKEND_ModelInitialize: 3_predicttensorflowtriton (version 1)\n",
      "I0922 05:19:30.002678 388 tensorflow.cc:2683] TRITONBACKEND_ModelInitialize: 1_predicttensorflowtriton (version 1)\n",
      "I0922 05:19:30.342921 388 tensorflow.cc:2732] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflowtriton_0_0 (CPU device 0)\n",
      "I0922 05:19:30.390247 388 tensorflow.cc:2732] TRITONBACKEND_ModelInstanceInitialize: 3_predicttensorflowtriton_0_0 (CPU device 0)\n",
      "I0922 05:19:30.452914 388 tensorflow.cc:2732] TRITONBACKEND_ModelInstanceInitialize: 1_predicttensorflowtriton_0_1 (CPU device 0)\n",
      "I0922 05:19:30.606804 388 model_lifecycle.cc:815] successfully loaded '1_predicttensorflowtriton'\n",
      "I0922 05:19:30.658043 388 tensorflow.cc:2732] TRITONBACKEND_ModelInstanceInitialize: 3_predicttensorflowtriton_0_1 (CPU device 0)\n",
      "I0922 05:19:30.974589 388 model_lifecycle.cc:815] successfully loaded '3_predicttensorflowtriton'\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "I0922 05:19:40.252016 388 python_be.cc:2055] TRITONBACKEND_ModelInstanceInitialize: 0_transformworkflowtriton_0 (CPU device 0)\n",
      "I0922 05:19:40.256787 388 python_be.cc:2055] TRITONBACKEND_ModelInstanceInitialize: executor_model_0 (CPU device 0)\n",
      "I0922 05:19:40.256841 388 python_be.cc:2055] TRITONBACKEND_ModelInstanceInitialize: 2_transformworkflowtriton_0 (CPU device 0)\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "I0922 05:19:45.759626 388 model_lifecycle.cc:815] successfully loaded '2_transformworkflowtriton'\n",
      "I0922 05:19:45.793719 388 model_lifecycle.cc:815] successfully loaded '0_transformworkflowtriton'\n",
      "I0922 05:19:47.967832 388 model_lifecycle.cc:815] successfully loaded 'executor_model'\n",
      "I0922 05:19:47.968039 388 server.cc:603] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0922 05:19:47.968104 388 server.cc:630] \n",
      "+------------+-------------------------------+-------------------------------+\n",
      "| Backend    | Path                          | Config                        |\n",
      "+------------+-------------------------------+-------------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/te | {\"cmdline\":{\"auto-complete-co |\n",
      "|            | nsorflow/libtriton_tensorflow | nfig\":\"true\",\"backend-directo |\n",
      "|            | .so                           | ry\":\"/opt/tritonserver/backen |\n",
      "|            |                               | ds\",\"min-compute-capability\": |\n",
      "|            |                               | \"6.000000\",\"version\":\"2\",\"def |\n",
      "|            |                               | ault-max-batch-size\":\"4\"}}    |\n",
      "|            |                               |                               |\n",
      "|            |                               |                               |\n",
      "| python     | /opt/tritonserver/backends/py | {\"cmdline\":{\"auto-complete-co |\n",
      "|            | thon/libtriton_python.so      | nfig\":\"true\",\"backend-directo |\n",
      "|            |                               | ry\":\"/opt/tritonserver/backen |\n",
      "|            |                               | ds\",\"min-compute-capability\": |\n",
      "|            |                               | \"6.000000\",\"default-max-batch |\n",
      "|            |                               | -size\":\"4\"}}                  |\n",
      "|            |                               |                               |\n",
      "+------------+-------------------------------+-------------------------------+\n",
      "\n",
      "I0922 05:19:47.968191 388 server.cc:673] \n",
      "+---------------------------+---------+--------+\n",
      "| Model                     | Version | Status |\n",
      "+---------------------------+---------+--------+\n",
      "| 0_transformworkflowtriton | 1       | READY  |\n",
      "| 1_predicttensorflowtriton | 1       | READY  |\n",
      "| 2_transformworkflowtriton | 1       | READY  |\n",
      "| 3_predicttensorflowtriton | 1       | READY  |\n",
      "| executor_model            | 1       | READY  |\n",
      "+---------------------------+---------+--------+\n",
      "\n",
      "I0922 05:19:47.968645 388 metrics.cc:701] Collecting CPU metrics\n",
      "I0922 05:19:47.969323 388 tritonserver.cc:2385] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.35.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /try-merlin/src/poc_ensemble/            |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0922 05:19:48.009711 388 grpc_server.cc:2445] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0922 05:19:48.009986 388 http_server.cc:3555] Started HTTPService at 0.0.0.0:8000\n",
      "I0922 05:19:48.054166 388 http_server.cc:185] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I0922 05:22:03.719085 388 server.cc:304] Waiting for in-flight requests to complete.\n",
      "I0922 05:22:03.719205 388 server.cc:320] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0922 05:22:03.719982 388 server.cc:335] All models are stopped, unloading models\n",
      "I0922 05:22:03.720051 388 server.cc:342] Timeout 30: Found 5 live models and 0 in-flight non-inference requests\n",
      "I0922 05:22:03.720128 388 tensorflow.cc:2770] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0922 05:22:03.720147 388 tensorflow.cc:2770] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0922 05:22:03.729319 388 tensorflow.cc:2770] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0922 05:22:03.730502 388 tensorflow.cc:2709] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0922 05:22:03.737538 388 model_lifecycle.cc:608] successfully unloaded '1_predicttensorflowtriton' version 1\n",
      "I0922 05:22:03.738977 388 tensorflow.cc:2770] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0922 05:22:03.739229 388 tensorflow.cc:2709] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0922 05:22:03.755385 388 model_lifecycle.cc:608] successfully unloaded '3_predicttensorflowtriton' version 1\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/try-merlin/poc_ensemble --backend-config=tensorflow,version=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78e22f18-6cdd-472e-8e1e-f023c244e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id\n",
      "0        7\n"
     ]
    }
   ],
   "source": [
    "# read in data for request\n",
    "from merlin.core.dispatch import make_df\n",
    "import numpy as np\n",
    "\n",
    "# create a request to be sent to TIS\n",
    "request = make_df({\"user_id\": [7]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a026227-7f63-48c7-8e83-79db2b52d35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ordered_ids': array([[360,  17,   2, 268, 114,   2, 296,   2, 139,   2]], dtype=int32),\n",
       " 'ordered_scores': array([[0.4980855 , 0.4982919 , 0.49818763, 0.49815258, 0.49854738,\n",
       "         0.49818763, 0.49851397, 0.49818763, 0.49850938, 0.49818763]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = send_triton_request(request_schema, request, outputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "845a45bd-07cf-4d75-80eb-c3ee7cac26c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id\n",
       "0        7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65604f3d-a8d2-4a18-99ca-3b5c5cce3731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
