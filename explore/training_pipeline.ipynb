{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66324df1-af67-4fa0-99fe-fc271f7b8d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/core/config.py:213: RuntimeWarning: Environment variable 'NUMBA_DISABLE_JIT' is defined but its associated value '\"1\"' could not be parsed.\n",
      "The parse failed with exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/core/config.py\", line 211, in _readenv\n",
      "    return ctor(value)\n",
      "ValueError: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "  warnings.warn(f\"Environment variable '{name}' is defined but \"\n",
      "2024-09-23 09:08:44.631242: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 09:08:44.668296: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Rename, Filter, Dropna, LambdaOp, Categorify, \\\n",
    "    TagAsUserFeatures, TagAsUserID, TagAsItemFeatures, TagAsItemID, AddMetadata\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.dag.ops.subgraph import Subgraph\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.datasets.ecommerce import transform_aliccp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ff73ad-2ab1-4e37-bd9e-12dc74d204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/try-merlin/data/\")\n",
    "# set up the base dir for feature store\n",
    "BASE_DIR = os.environ.get(\n",
    "    \"BASE_DIR\", \"/try-merlin/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a7655-af88-4970-8877-258e581b5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 100_000)\n",
    "train_raw, valid_raw = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ffb79-fb46-486f-a5ad-e1fddb74f72e",
   "metadata": {},
   "source": [
    "If you would like to use the real ALI-CCP dataset, you can use [get_aliccp()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/datasets/ecommerce/aliccp/dataset.py) function instead. This function takes the raw csv files, and generate parquet files that can be directly fed to NVTabular workflow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec19d36-c707-454c-a02e-9e874a578122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac06fd0-dd35-4c34-a8be-f53938502234",
   "metadata": {},
   "source": [
    "## Set up a feature store with Feast\n",
    "Before we move onto the next step, we need to create a Feast feature repository.[Feast](https://feast.dev/)t is an end-to-end open source feature store for machine learning. Feast (Feature Store) is a customizable operational data system that re-uses existing infrastructure to manage and serve machine learning features to real-time models.\n",
    "\n",
    "We will create the feature repo in the current working directory, which `BASE_DIR`DIR for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10a190d-62b8-4235-9bee-6a7cb1a595ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/try-merlin/feast_repo\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $BASE_DIR/feast_repo\n",
    "!cd $BASE_DIR && feast init feast_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1d39db-6610-4d24-a5ab-580b84892efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo\")\n",
    "if os.path.exists(f\"{feature_repo_path}/example_repo.py\"):\n",
    "    os.remove(f\"{feature_repo_path}/example_repo.py\")\n",
    "if os.path.exists(f\"{feature_repo_path}/data/driver_stats.parquet\"):\n",
    "    os.remove(f\"{feature_repo_path}/data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20807ba-cbb0-426b-8c71-f854cacb7e4f",
   "metadata": {},
   "source": [
    "## Exporting user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becb374b-6f69-4f09-bd16-f19d1d807d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<merlin.io.dataset.Dataset at 0x7f4bfa7ffac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "unique_rows_by_features(train_raw, Tags.USER, Tags.USER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebda30a7-961c-4a4c-8c4d-9bfec7598f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "user_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.USER, Tags.USER_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27805d65-6fc8-43d6-af10-18c1beac7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "user_features[\"datetime\"] = datetime.now()\n",
    "user_features[\"datetime\"] = user_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "user_features[\"created\"] = datetime.now()\n",
    "user_features[\"created\"] = user_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1bbe23-35fe-42bd-8cbf-d2586bbc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_shops</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>user_group</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_consumption_1</th>\n",
       "      <th>user_consumption_2</th>\n",
       "      <th>user_is_occupied</th>\n",
       "      <th>user_geography</th>\n",
       "      <th>user_intentions</th>\n",
       "      <th>user_brands</th>\n",
       "      <th>user_categories</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>479</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>238</td>\n",
       "      <td>25</td>\n",
       "      <td>2024-09-23 09:08:51.335918</td>\n",
       "      <td>2024-09-23 09:08:51.338844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_shops  user_profile  user_group  user_gender  user_age  \\\n",
       "0        7         479             1           1            1         1   \n",
       "\n",
       "   user_consumption_1  user_consumption_2  user_is_occupied  user_geography  \\\n",
       "0                   1                   1                 1               1   \n",
       "\n",
       "   user_intentions  user_brands  user_categories                   datetime  \\\n",
       "0              139          238               25 2024-09-23 09:08:51.335918   \n",
       "\n",
       "                     created  \n",
       "0 2024-09-23 09:08:51.338844  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features[user_features['user_id']  == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067b1e25-6472-45df-9cae-68dbb3101bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"user_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c06bae-2410-44b7-99d6-4067036d5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "item_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.ITEM, Tags.ITEM_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103877e4-c4ab-4a38-8614-ec630ea92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features[\"datetime\"] = datetime.now()\n",
    "item_features[\"datetime\"] = item_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "item_features[\"created\"] = datetime.now()\n",
    "item_features[\"created\"] = item_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33e2274-ca14-4034-826b-aeb454663a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category</th>\n",
       "      <th>item_shop</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>item_intention</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>84</td>\n",
       "      <td>5869</td>\n",
       "      <td>2022</td>\n",
       "      <td>935</td>\n",
       "      <td>2024-09-23 09:08:51.410932</td>\n",
       "      <td>2024-09-23 09:08:51.413514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>1806</td>\n",
       "      <td>622</td>\n",
       "      <td>288</td>\n",
       "      <td>2024-09-23 09:08:51.410932</td>\n",
       "      <td>2024-09-23 09:08:51.413514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>398</td>\n",
       "      <td>27991</td>\n",
       "      <td>9640</td>\n",
       "      <td>4458</td>\n",
       "      <td>2024-09-23 09:08:51.410932</td>\n",
       "      <td>2024-09-23 09:08:51.413514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>903</td>\n",
       "      <td>311</td>\n",
       "      <td>144</td>\n",
       "      <td>2024-09-23 09:08:51.410932</td>\n",
       "      <td>2024-09-23 09:08:51.413514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>372</td>\n",
       "      <td>26185</td>\n",
       "      <td>9018</td>\n",
       "      <td>4170</td>\n",
       "      <td>2024-09-23 09:08:51.410932</td>\n",
       "      <td>2024-09-23 09:08:51.413514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  item_category  item_shop  item_brand  item_intention  \\\n",
       "0       14             84       5869        2022             935   \n",
       "1        5             26       1806         622             288   \n",
       "2       63            398      27991        9640            4458   \n",
       "3        3             13        903         311             144   \n",
       "4       59            372      26185        9018            4170   \n",
       "\n",
       "                    datetime                    created  \n",
       "0 2024-09-23 09:08:51.410932 2024-09-23 09:08:51.413514  \n",
       "1 2024-09-23 09:08:51.410932 2024-09-23 09:08:51.413514  \n",
       "2 2024-09-23 09:08:51.410932 2024-09-23 09:08:51.413514  \n",
       "3 2024-09-23 09:08:51.410932 2024-09-23 09:08:51.413514  \n",
       "4 2024-09-23 09:08:51.410932 2024-09-23 09:08:51.413514  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588a6794-f801-4d68-aa09-0f0d89e399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"item_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9dd05-fe8d-444a-ba7c-183b15280ff2",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818a5ab4-736a-4712-af0e-7789112d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_FOLDER, \"processed_nvt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41e15285-048a-4b23-81a3-b52111c4084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the user_id and item_id to the pipeline: Rename (add postfix \"_raw\") -> Casttype. And Mark these as Feature of Recsys\n",
    "user_id_raw = [\"user_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsUserFeatures()\n",
    "item_id_raw = [\"item_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsItemFeatures()\n",
    "\n",
    "# Feed the item_id, item_category, item_shop, item_brand into the Categorify processing.\n",
    "item_cat = Categorify(dtype=\"int32\")\n",
    "items = ([\"item_id\",\"item_category\", \"item_shop\", \"item_brand\"] >> item_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791b279-e74d-4d8f-9e97-bc87acbbfaee",
   "metadata": {},
   "source": [
    "column_mapping\r\n",
    "\r\n",
    "input_dtypes\r\n",
    "\r\n",
    "input_schema\r\n",
    "\r\n",
    "leaf_nodes\r\n",
    "\r\n",
    "output_dtypes\r\n",
    "\r\n",
    "output_schemaput_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c9c02a-2bf6-4e40-bae0-7d36c378c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_item = Subgraph(\n",
    "     \"item\", \n",
    "     Subgraph(\"items_cat\", items) + \n",
    "    (items[\"item_id\"] >> TagAsItemID()) + \n",
    "    (items[\"item_category\", \"item_shop\", \"item_brand\"] >> TagAsItemFeatures())\n",
    ")\n",
    "subgraph_user = Subgraph(\n",
    "    \"user\",\n",
    "    ([\"user_id\"] >> Categorify(dtype=\"int32\") >> TagAsUserID()) +\n",
    "    (\n",
    "        [\n",
    "            \"user_shops\",\n",
    "            \"user_profile\",\n",
    "            \"user_group\",\n",
    "            \"user_gender\",\n",
    "            \"user_age\",\n",
    "            \"user_consumption_2\",\n",
    "            \"user_is_occupied\",\n",
    "            \"user_geography\",\n",
    "            \"user_intentions\",\n",
    "            \"user_brands\",\n",
    "            \"user_categories\",\n",
    "        ] >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fefe559f-f285-47d8-aaec-70ac86609201",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "outputs = subgraph_user + subgraph_item + targets\n",
    "\n",
    "# add dropna op to filter rows with nulls\n",
    "outputs = outputs >> Dropna()\n",
    "nvt_wkflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f866d59c-0ce5-4e7b-91c2-20faabf4a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"414pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 413.79 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-400 409.79,-400 409.79,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"55.9\" cy=\"-234\" rx=\"55.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Subgraph</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86,-218.67C111.17,-206.68 146.93,-189.65 172.25,-177.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.88,-180.69 181.41,-173.23 170.87,-174.37 173.88,-180.69\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-90\" rx=\"44.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Dropna</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.9,-71.7C202.9,-63.98 202.9,-54.71 202.9,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.4,-46.1 202.9,-36.1 199.4,-46.1 206.4,-46.1\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.9,-143.7C202.9,-135.98 202.9,-126.71 202.9,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.4,-118.1 202.9,-108.1 199.4,-118.1 206.4,-118.1\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-234\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">AddMetadata</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.9,-215.7C202.9,-207.98 202.9,-198.71 202.9,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.4,-190.1 202.9,-180.1 199.4,-190.1 206.4,-190.1\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-306\" rx=\"66.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">SelectionOp</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.9,-287.7C202.9,-279.98 202.9,-270.71 202.9,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.4,-262.1 202.9,-252.1 199.4,-262.1 206.4,-262.1\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"349.9\" cy=\"-234\" rx=\"55.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"349.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Subgraph</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M319.79,-218.67C294.62,-206.68 258.86,-189.65 233.54,-177.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234.92,-174.37 224.38,-173.23 231.91,-180.69 234.92,-174.37\"/>\n",
       "</g>\n",
       "<!-- 5_selector -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5_selector</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"202.9\" cy=\"-378\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.9\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">[&#39;click&#39;]</text>\n",
       "</g>\n",
       "<!-- 5_selector&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5_selector&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.9,-359.7C202.9,-351.98 202.9,-342.71 202.9,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.4,-334.1 202.9,-324.1 199.4,-334.1 206.4,-334.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f4bfa31db10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbd354-dfe6-46ac-9adf-250212c5565b",
   "metadata": {},
   "source": [
    "Let’s call transform_aliccp utility function to be able to perform fit and transform steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65801f3a-8cfc-48f1-8908-c10b44b74cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform_aliccp(\n",
    "    (train_raw, valid_raw), output_path, nvt_workflow=nvt_wkflow, workflow_name=\"workflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b454836-2caf-4d8f-85b4-a912bfabb92a",
   "metadata": {},
   "source": [
    "## Training a Retrieval Model with Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70eedd-021d-4ac5-99b8-e4cd1c9b8689",
   "metadata": {},
   "source": [
    "We start with the offline candidate retrieval stage. We are going to train a Two-Tower model for item retrieval. To learn more about the Two-tower model you can visit [05-Retrieval-Model.ipynb](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b90b28-c71a-4477-a996-7fd0204ad5ef",
   "metadata": {},
   "source": [
    "We are going to process our raw categorical features by encoding them using Categorify() operator and tag the features with user or item tags in the schema file. To learn more about NVTabular and the schema object visit this example notebook in the Merlin Models repo. \\\n",
    "Define a new output path to store the filtered datasets and schema files.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/NVTabular \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4cf2fd7-67d8-473b-a349-cc9a38ef8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path2 = os.path.join(DATA_FOLDER, \"processed/retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eb5a0f7-e02a-4b41-b664-38e627416f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"))\n",
    "valid_tt = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9dc47b4-d99c-4809-a012-27abe5c201d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = train_tt.schema.column_names\n",
    "outputs = inputs >> Filter(f=lambda df: df[\"click\"] == 1)\n",
    "\n",
    "nvt_wkflow = nvt.Workflow(outputs)\n",
    "nvt_wkflow.fit(train_tt)\n",
    "\n",
    "nvt_wkflow.transform(train_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"train\")\n",
    ")\n",
    "\n",
    "nvt_wkflow.transform(valid_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6c95b-3b24-463b-b587-147b9c019f76",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file, schema.pbtxt a protobuf text file, of our processed dataset. To learn more about the schema object and schema file you can explore 02-Merlin-Models-and-NVTabular-integration.ipynb notebook.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8187a40-30a9-4bd9-9e3a-b9c595b3f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path2, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid_tt = Dataset(os.path.join(output_path2, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa62a9aa-e47a-404a-bb66-01f0f477474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train_tt.schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER]).without(['click'])\n",
    "train_tt.schema = schema\n",
    "valid_tt.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c4b6bec-47d3-43c9-92a7-d0f7b6e31bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tt = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b580d34-525f-4064-bcda-5684e2d2d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2024-09-23 09:09:49.057540: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - ETA: 0s - loss: 8.9540 - recall_at_10: 0.0087 - ndcg_at_10: 0.0064 - regularization_loss: 0.0000e+00 - loss_batch: 8.9253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 09:10:09.246214: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 24s 1s/step - loss: 8.9540 - recall_at_10: 0.0090 - ndcg_at_10: 0.0065 - regularization_loss: 0.0000e+00 - loss_batch: 8.8713 - val_loss: 8.9182 - val_recall_at_10: 0.0176 - val_ndcg_at_10: 0.0121 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 8.5809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4bf80be830>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tt.compile(\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=False,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)],\n",
    ")\n",
    "model_tt.fit(train_tt, validation_data=valid_tt, batch_size=1024 * 8, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "743d9d98-81e8-4d98-bb5d-c26816972c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tower = model_tt.retrieval_block.query_block()\n",
    "query_tower.save(os.path.join(BASE_DIR, \"query_tower\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934917ea-d24f-49c9-9e9e-ebcaec9d95dd",
   "metadata": {},
   "source": [
    "## Training a Ranking Model with DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f425d1b-3d5a-44c3-bb16-da3415506dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a40bcf0-8177-4e67-96f8-4351302a0690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000b4f3-3695-4fe6-8b85-c438809fc342",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model (DLRM) architecture is a popular neural network model originally proposed by Facebook in 2019. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in here. To learn more about DLRM architetcture please visit `Exploring-different-models` notebook in the Merlin Models GH repo.\n",
    "\n",
    "https://arxiv.org/abs/1906.00091 \\\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5694074 \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/04-Exporting-ranking-models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0609d305-9028-46e9-a9cd-eaa5852497aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c873dabd-1b6c-48b9-bef7-5a3521445344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 09:10:19.935869: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 [=======================>......] - ETA: 0s - loss: 0.6932 - auc: 0.4999 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 09:10:25.316046: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 6s 424ms/step - loss: 0.6932 - auc: 0.4993 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932 - val_loss: 0.6932 - val_auc: 0.5020 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b8b6e0130>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=16 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "187c8421-2efd-4ed4-87cf-c069b56c6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(BASE_DIR, \"dlrm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80223f-ac70-44f9-8cd6-59f859a78a6d",
   "metadata": {},
   "source": [
    "In the following cells we are going to export the required user and item features files, and save the query (user) tower model and item embeddings to disk. If you want to read more about exporting retrieval models, please visit 05-Retrieval-Model.ipynb notebook in Merlin Models library repo.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81b388-8d64-437f-836d-7b6c6d2deb2c",
   "metadata": {},
   "source": [
    "## Extract and save Item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "655af406-0af4-40d2-8e04-cc7ba6475ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "workflow =  nvt.Workflow([\"item_id\"] + (['item_id', 'item_brand', 'item_category', 'item_shop'] >> TransformWorkflow(nvt_wkflow.get_subworkflow(\"item\")) >> PredictTensorflow(model_tt.first.item_block())))\n",
    "item_embeddings = workflow.fit_transform(Dataset(item_features)).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c889750-5ac0-4e22-918f-d1e1c64d2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     item_id                                           output_1\n",
      "448      613  [-0.03930280730128288, -0.0114214476197958, -0...\n",
      "449      331  [-0.07048268616199493, -0.033806201070547104, ...\n",
      "450      403  [-0.059938959777355194, -0.03728408366441727, ...\n",
      "451     1009  [-0.03930280730128288, -0.0114214476197958, -0...\n",
      "452      361  [-0.05127168446779251, -0.03720864653587341, 0...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(item_embeddings.tail())\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bbbf872-4668-4ba9-919f-5500bc88e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embeddings.to_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88c7a-c955-45a8-aa87-ad29b7b4012f",
   "metadata": {},
   "source": [
    "## Create feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0a0c007-7f01-4be6-9b3d-bca277fe95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /try-merlin/feast_repo/feature_repo/user_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/feature_repo/user_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "user_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/feature_repo/data/user_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "user = Entity(name=\"user_id\", value_type=ValueType.INT32, join_keys=[\"user_id\"],)\n",
    "\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[user],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"user_shops\", dtype=Int32),\n",
    "        Field(name=\"user_profile\", dtype=Int32),\n",
    "        Field(name=\"user_group\", dtype=Int32),\n",
    "        Field(name=\"user_gender\", dtype=Int32),\n",
    "        Field(name=\"user_age\", dtype=Int32),\n",
    "        Field(name=\"user_consumption_2\", dtype=Int32),\n",
    "        Field(name=\"user_is_occupied\", dtype=Int32),\n",
    "        Field(name=\"user_geography\", dtype=Int32),\n",
    "        Field(name=\"user_intentions\", dtype=Int32),\n",
    "        Field(name=\"user_brands\", dtype=Int32),\n",
    "        Field(name=\"user_categories\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=user_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6afb5241-346f-413a-825d-c76b4c888f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /try-merlin/feast_repo/feature_repo/item_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/feature_repo/item_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/feature_repo/data/item_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"item_id\", value_type=ValueType.INT32, join_keys=[\"item_id\"],)\n",
    "\n",
    "item_features_view = FeatureView(\n",
    "    name=\"item_features\",\n",
    "    entities=[item],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"item_category\", dtype=Int32),\n",
    "        Field(name=\"item_shop\", dtype=Int32),\n",
    "        Field(name=\"item_brand\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=item_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23663ade-7a0d-450b-924b-285a34a8486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feast_repo/\n",
      "├─README.md\n",
      "├─__init__.py\n",
      "└─feature_repo/\n",
      "  ├─__init__.py\n",
      "  ├─__pycache__/\n",
      "  │ ├─__init__.cpython-310.pyc\n",
      "  │ ├─example_repo.cpython-310.pyc\n",
      "  │ └─test_workflow.cpython-310.pyc\n",
      "  ├─data/\n",
      "  │ ├─item_features.parquet\n",
      "  │ └─user_features.parquet\n",
      "  ├─feature_store.yaml\n",
      "  ├─item_features.py\n",
      "  ├─test_workflow.py\n",
      "  └─user_features.py\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo\")\n",
    "sd.seedir(\n",
    "    feature_repo_path,\n",
    "    style=\"lines\",\n",
    "    itemlimit=10,\n",
    "    depthlimit=3,\n",
    "    exclude_folders=\".ipynb_checkpoints\",\n",
    "    sort=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cf2d0-ac85-49ef-a022-e09e6dc76b91",
   "metadata": {},
   "source": [
    "We trained and exported our ranking and retrieval models and NVTabular workflows. In the next step, we will learn how to deploy our trained models into Triton Inference Server (TIS) with Merlin Systems library.\n",
    "\n",
    "For the next step, move on to the 02-Deploying-multi-stage-Recsys-with-Merlin-Systems.ipynb notebook to deploy our saved models as an ensemble to TIS and obtain prediction results for a given reques\n",
    "\n",
    "https://github.com/triton-inference-server/servert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2ef13-95c8-491c-8645-b8f41bf6af0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
