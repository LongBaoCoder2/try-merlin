{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66324df1-af67-4fa0-99fe-fc271f7b8d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/core/config.py:213: RuntimeWarning: Environment variable 'NUMBA_DISABLE_JIT' is defined but its associated value '\"1\"' could not be parsed.\n",
      "The parse failed with exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/core/config.py\", line 211, in _readenv\n",
      "    return ctor(value)\n",
      "ValueError: invalid literal for int() with base 10: '\"1\"'\n",
      "\n",
      "  warnings.warn(f\"Environment variable '{name}' is defined but \"\n",
      "2024-09-22 06:47:11.271557: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-22 06:47:12.095790: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Rename, Filter, Dropna, LambdaOp, Categorify, \\\n",
    "    TagAsUserFeatures, TagAsUserID, TagAsItemFeatures, TagAsItemID, AddMetadata\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.dag.ops.subgraph import Subgraph\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.datasets.ecommerce import transform_aliccp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ff73ad-2ab1-4e37-bd9e-12dc74d204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/try-merlin/data/\")\n",
    "# set up the base dir for feature store\n",
    "BASE_DIR = os.environ.get(\n",
    "    \"BASE_DIR\", \"/try-merlin/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a7655-af88-4970-8877-258e581b5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 100_000)\n",
    "train_raw, valid_raw = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ffb79-fb46-486f-a5ad-e1fddb74f72e",
   "metadata": {},
   "source": [
    "If you would like to use the real ALI-CCP dataset, you can use [get_aliccp()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/datasets/ecommerce/aliccp/dataset.py) function instead. This function takes the raw csv files, and generate parquet files that can be directly fed to NVTabular workflow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec19d36-c707-454c-a02e-9e874a578122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac06fd0-dd35-4c34-a8be-f53938502234",
   "metadata": {},
   "source": [
    "## Set up a feature store with Feast\n",
    "Before we move onto the next step, we need to create a Feast feature repository.[Feast](https://feast.dev/)t is an end-to-end open source feature store for machine learning. Feast (Feature Store) is a customizable operational data system that re-uses existing infrastructure to manage and serve machine learning features to real-time models.\n",
    "\n",
    "We will create the feature repo in the current working directory, which `BASE_DIR`DIR for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10a190d-62b8-4235-9bee-6a7cb1a595ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/try-merlin/feast_repo\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $BASE_DIR/feast_repo\n",
    "!cd $BASE_DIR && feast init feast_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1d39db-6610-4d24-a5ab-580b84892efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo\")\n",
    "if os.path.exists(f\"{feature_repo_path}/example_repo.py\"):\n",
    "    os.remove(f\"{feature_repo_path}/example_repo.py\")\n",
    "if os.path.exists(f\"{feature_repo_path}/data/driver_stats.parquet\"):\n",
    "    os.remove(f\"{feature_repo_path}/data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20807ba-cbb0-426b-8c71-f854cacb7e4f",
   "metadata": {},
   "source": [
    "## Exporting user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becb374b-6f69-4f09-bd16-f19d1d807d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<merlin.io.dataset.Dataset at 0x7f9ef99fbee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "unique_rows_by_features(train_raw, Tags.USER, Tags.USER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebda30a7-961c-4a4c-8c4d-9bfec7598f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "user_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.USER, Tags.USER_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27805d65-6fc8-43d6-af10-18c1beac7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "user_features[\"datetime\"] = datetime.now()\n",
    "user_features[\"datetime\"] = user_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "user_features[\"created\"] = datetime.now()\n",
    "user_features[\"created\"] = user_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1bbe23-35fe-42bd-8cbf-d2586bbc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_shops</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>user_group</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_consumption_1</th>\n",
       "      <th>user_consumption_2</th>\n",
       "      <th>user_is_occupied</th>\n",
       "      <th>user_geography</th>\n",
       "      <th>...</th>\n",
       "      <th>item_shop</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>item_intention</th>\n",
       "      <th>user_item_categories</th>\n",
       "      <th>user_item_shops</th>\n",
       "      <th>user_item_brands</th>\n",
       "      <th>user_item_intentions</th>\n",
       "      <th>position</th>\n",
       "      <th>click</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6918</td>\n",
       "      <td>2383</td>\n",
       "      <td>1102</td>\n",
       "      <td>331</td>\n",
       "      <td>194111</td>\n",
       "      <td>38662</td>\n",
       "      <td>6784</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>434</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1153</td>\n",
       "      <td>398</td>\n",
       "      <td>184</td>\n",
       "      <td>5259</td>\n",
       "      <td>118163</td>\n",
       "      <td>115040</td>\n",
       "      <td>72901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>674</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11914</td>\n",
       "      <td>4103</td>\n",
       "      <td>1897</td>\n",
       "      <td>7466</td>\n",
       "      <td>196296</td>\n",
       "      <td>7256</td>\n",
       "      <td>22856</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>61104</td>\n",
       "      <td>21043</td>\n",
       "      <td>9730</td>\n",
       "      <td>783</td>\n",
       "      <td>301873</td>\n",
       "      <td>2691</td>\n",
       "      <td>60145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4996</td>\n",
       "      <td>1721</td>\n",
       "      <td>796</td>\n",
       "      <td>1336</td>\n",
       "      <td>322633</td>\n",
       "      <td>114312</td>\n",
       "      <td>21063</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>18</td>\n",
       "      <td>819</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>41504</td>\n",
       "      <td>14294</td>\n",
       "      <td>6609</td>\n",
       "      <td>2603</td>\n",
       "      <td>34121</td>\n",
       "      <td>66339</td>\n",
       "      <td>20654</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>57</td>\n",
       "      <td>2696</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5765</td>\n",
       "      <td>1986</td>\n",
       "      <td>918</td>\n",
       "      <td>6496</td>\n",
       "      <td>88300</td>\n",
       "      <td>78572</td>\n",
       "      <td>67470</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>16</td>\n",
       "      <td>723</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1538</td>\n",
       "      <td>530</td>\n",
       "      <td>245</td>\n",
       "      <td>540</td>\n",
       "      <td>275375</td>\n",
       "      <td>137510</td>\n",
       "      <td>43492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11529</td>\n",
       "      <td>3971</td>\n",
       "      <td>1836</td>\n",
       "      <td>4458</td>\n",
       "      <td>239815</td>\n",
       "      <td>84287</td>\n",
       "      <td>2668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>59</td>\n",
       "      <td>2793</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>21521</td>\n",
       "      <td>7412</td>\n",
       "      <td>3427</td>\n",
       "      <td>963</td>\n",
       "      <td>69623</td>\n",
       "      <td>8047</td>\n",
       "      <td>51877</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  user_shops  user_profile  user_group  user_gender  user_age  \\\n",
       "0            9         386             1           1            1         1   \n",
       "1           10         434             1           1            1         1   \n",
       "2           15         674             1           1            1         1   \n",
       "3           11         482             1           1            1         1   \n",
       "4            6         241             1           1            1         1   \n",
       "...        ...         ...           ...         ...          ...       ...   \n",
       "69995       18         819             1           1            1         1   \n",
       "69996       57        2696             3           1            1         1   \n",
       "69997       16         723             1           1            1         1   \n",
       "69998        3          97             1           1            1         1   \n",
       "69999       59        2793             3           1            1         1   \n",
       "\n",
       "       user_consumption_1  user_consumption_2  user_is_occupied  \\\n",
       "0                       1                   1                 1   \n",
       "1                       1                   1                 1   \n",
       "2                       1                   1                 1   \n",
       "3                       1                   1                 1   \n",
       "4                       1                   1                 1   \n",
       "...                   ...                 ...               ...   \n",
       "69995                   1                   1                 1   \n",
       "69996                   1                   1                 1   \n",
       "69997                   1                   1                 1   \n",
       "69998                   1                   1                 1   \n",
       "69999                   1                   1                 1   \n",
       "\n",
       "       user_geography  ...  item_shop  item_brand  item_intention  \\\n",
       "0                   1  ...       6918        2383            1102   \n",
       "1                   1  ...       1153         398             184   \n",
       "2                   1  ...      11914        4103            1897   \n",
       "3                   1  ...      61104       21043            9730   \n",
       "4                   1  ...       4996        1721             796   \n",
       "...               ...  ...        ...         ...             ...   \n",
       "69995               1  ...      41504       14294            6609   \n",
       "69996               1  ...       5765        1986             918   \n",
       "69997               1  ...       1538         530             245   \n",
       "69998               1  ...      11529        3971            1836   \n",
       "69999               1  ...      21521        7412            3427   \n",
       "\n",
       "       user_item_categories  user_item_shops  user_item_brands  \\\n",
       "0                       331           194111             38662   \n",
       "1                      5259           118163            115040   \n",
       "2                      7466           196296              7256   \n",
       "3                       783           301873              2691   \n",
       "4                      1336           322633            114312   \n",
       "...                     ...              ...               ...   \n",
       "69995                  2603            34121             66339   \n",
       "69996                  6496            88300             78572   \n",
       "69997                   540           275375            137510   \n",
       "69998                  4458           239815             84287   \n",
       "69999                   963            69623              8047   \n",
       "\n",
       "       user_item_intentions  position  click  conversion  \n",
       "0                      6784         0      1           1  \n",
       "1                     72901         0      1           1  \n",
       "2                     22856         3      0           0  \n",
       "3                     60145         2      1           0  \n",
       "4                     21063         2      1           0  \n",
       "...                     ...       ...    ...         ...  \n",
       "69995                 20654         1      0           1  \n",
       "69996                 67470         2      0           0  \n",
       "69997                 43492         1      1           1  \n",
       "69998                  2668         0      0           0  \n",
       "69999                 51877         0      1           1  \n",
       "\n",
       "[70000 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features[user_features['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "067b1e25-6472-45df-9cae-68dbb3101bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"user_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2c06bae-2410-44b7-99d6-4067036d5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "item_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.ITEM, Tags.ITEM_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "103877e4-c4ab-4a38-8614-ec630ea92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features[\"datetime\"] = datetime.now()\n",
    "item_features[\"datetime\"] = item_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "item_features[\"created\"] = datetime.now()\n",
    "item_features[\"created\"] = item_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f33e2274-ca14-4034-826b-aeb454663a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category</th>\n",
       "      <th>item_shop</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>item_intention</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>6918</td>\n",
       "      <td>2383</td>\n",
       "      <td>1102</td>\n",
       "      <td>2024-09-22 06:58:11.766741</td>\n",
       "      <td>2024-09-22 06:58:11.768123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1153</td>\n",
       "      <td>398</td>\n",
       "      <td>184</td>\n",
       "      <td>2024-09-22 06:58:11.766741</td>\n",
       "      <td>2024-09-22 06:58:11.768123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>170</td>\n",
       "      <td>11914</td>\n",
       "      <td>4103</td>\n",
       "      <td>1897</td>\n",
       "      <td>2024-09-22 06:58:11.766741</td>\n",
       "      <td>2024-09-22 06:58:11.768123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160</td>\n",
       "      <td>868</td>\n",
       "      <td>61104</td>\n",
       "      <td>21043</td>\n",
       "      <td>9730</td>\n",
       "      <td>2024-09-22 06:58:11.766741</td>\n",
       "      <td>2024-09-22 06:58:11.768123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4996</td>\n",
       "      <td>1721</td>\n",
       "      <td>796</td>\n",
       "      <td>2024-09-22 06:58:11.766741</td>\n",
       "      <td>2024-09-22 06:58:11.768123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  item_category  item_shop  item_brand  item_intention  \\\n",
       "0       19             99       6918        2383            1102   \n",
       "1        4             17       1153         398             184   \n",
       "2       32            170      11914        4103            1897   \n",
       "3      160            868      61104       21043            9730   \n",
       "4       14             71       4996        1721             796   \n",
       "\n",
       "                    datetime                    created  \n",
       "0 2024-09-22 06:58:11.766741 2024-09-22 06:58:11.768123  \n",
       "1 2024-09-22 06:58:11.766741 2024-09-22 06:58:11.768123  \n",
       "2 2024-09-22 06:58:11.766741 2024-09-22 06:58:11.768123  \n",
       "3 2024-09-22 06:58:11.766741 2024-09-22 06:58:11.768123  \n",
       "4 2024-09-22 06:58:11.766741 2024-09-22 06:58:11.768123  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588a6794-f801-4d68-aa09-0f0d89e399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"item_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9dd05-fe8d-444a-ba7c-183b15280ff2",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "818a5ab4-736a-4712-af0e-7789112d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_FOLDER, \"processed_nvt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41e15285-048a-4b23-81a3-b52111c4084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the user_id and item_id to the pipeline: Rename (add postfix \"_raw\") -> Casttype. And Mark these as Feature of Recsys\n",
    "user_id_raw = [\"user_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsUserFeatures()\n",
    "item_id_raw = [\"item_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsItemFeatures()\n",
    "\n",
    "# Feed the item_id, item_category, item_shop, item_brand into the Categorify processing.\n",
    "item_cat = Categorify(dtype=\"int32\")\n",
    "items = ([\"item_id\",\"item_category\", \"item_shop\", \"item_brand\"] >> item_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791b279-e74d-4d8f-9e97-bc87acbbfaee",
   "metadata": {},
   "source": [
    "column_mapping\r\n",
    "\r\n",
    "input_dtypes\r\n",
    "\r\n",
    "input_schema\r\n",
    "\r\n",
    "leaf_nodes\r\n",
    "\r\n",
    "output_dtypes\r\n",
    "\r\n",
    "output_schemaput_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94c9c02a-2bf6-4e40-bae0-7d36c378c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_item = Subgraph(\n",
    "     \"item\", \n",
    "     Subgraph(\"items_cat\", items) + \n",
    "    (items[\"item_id\"] >> TagAsItemID()) + \n",
    "    (items[\"item_category\", \"item_shop\", \"item_brand\"] >> TagAsItemFeatures())\n",
    ")\n",
    "subgraph_user = Subgraph(\n",
    "    \"user\",\n",
    "    ([\"user_id\"] >> Categorify(dtype=\"int32\") >> TagAsUserID()) +\n",
    "    (\n",
    "        [\n",
    "            \"user_shops\",\n",
    "            \"user_profile\",\n",
    "            \"user_group\",\n",
    "            \"user_gender\",\n",
    "            \"user_age\",\n",
    "            \"user_consumption_2\",\n",
    "            \"user_is_occupied\",\n",
    "            \"user_geography\",\n",
    "            \"user_intentions\",\n",
    "            \"user_brands\",\n",
    "            \"user_categories\",\n",
    "        ] >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fefe559f-f285-47d8-aaec-70ac86609201",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "outputs = subgraph_user + subgraph_item + targets\n",
    "\n",
    "# add dropna op to filter rows with nulls\n",
    "outputs = outputs >> Dropna()\n",
    "nvt_wkflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f866d59c-0ce5-4e7b-91c2-20faabf4a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"414pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 414.34 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-400 410.34,-400 410.34,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"332.9\" cy=\"-306\" rx=\"66.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"332.9\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">SelectionOp</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"332.9\" cy=\"-234\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"332.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">AddMetadata</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M332.9,-287.7C332.9,-279.98 332.9,-270.71 332.9,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"336.4,-262.1 332.9,-252.1 329.4,-262.1 336.4,-262.1\"/>\n",
       "</g>\n",
       "<!-- 0_selector -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>0_selector</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"332.9\" cy=\"-378\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"332.9\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">[&#39;click&#39;]</text>\n",
       "</g>\n",
       "<!-- 0_selector&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0_selector&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M332.9,-359.7C332.9,-351.98 332.9,-342.71 332.9,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"336.4,-334.1 332.9,-324.1 329.4,-334.1 336.4,-334.1\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185.9\" cy=\"-90\" rx=\"44.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.9\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Dropna</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185.9\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.9\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">output cols</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.9,-71.7C185.9,-63.98 185.9,-54.71 185.9,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.4,-46.1 185.9,-36.1 182.4,-46.1 189.4,-46.1\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185.9\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.9\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.9,-143.7C185.9,-135.98 185.9,-126.71 185.9,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.4,-118.1 185.9,-108.1 182.4,-118.1 189.4,-118.1\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"55.9\" cy=\"-234\" rx=\"55.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Subgraph</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83.46,-218.15C104.99,-206.57 134.81,-190.51 156.73,-178.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.45,-181.76 165.59,-173.93 155.13,-175.59 158.45,-181.76\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185.9\" cy=\"-234\" rx=\"55.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.9\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Subgraph</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.9,-215.7C185.9,-207.98 185.9,-198.71 185.9,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.4,-190.1 185.9,-180.1 182.4,-190.1 189.4,-190.1\"/>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.64,-217.64C275.57,-205.7 241.03,-189.26 216.42,-177.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.72,-174.28 207.18,-173.14 214.71,-180.6 217.72,-174.28\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9ef96010c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbd354-dfe6-46ac-9adf-250212c5565b",
   "metadata": {},
   "source": [
    "Let’s call transform_aliccp utility function to be able to perform fit and transform steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65801f3a-8cfc-48f1-8908-c10b44b74cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "Failed to fit operator <merlin.dag.ops.subgraph.Subgraph object at 0x7f9ef95a8070>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 530, in fit_phase\n",
      "    stats.append(node.op.fit(node.input_columns, Dataset(ddf)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/ops/subgraph.py\", line 75, in fit\n",
      "    DaskExecutor().fit(dataset, self.graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 466, in fit\n",
      "    self.fit_phase(dataset, current_phase)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 520, in fit_phase\n",
      "    transformed_ddf = self.transform(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 401, in transform\n",
      "    return ensure_optimize_dataframe_graph(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/core/utils.py\", line 134, in ensure_optimize_dataframe_graph\n",
      "    ddf.dask = dsk\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/dask_expr/_collection.py\", line 3061, in __setattr__\n",
      "    object.__setattr__(self, key, value)\n",
      "AttributeError: can't set attribute 'dask'\n",
      "Failed to fit operator <merlin.dag.ops.subgraph.Subgraph object at 0x7f9ef95c2c50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 530, in fit_phase\n",
      "    stats.append(node.op.fit(node.input_columns, Dataset(ddf)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/ops/subgraph.py\", line 75, in fit\n",
      "    DaskExecutor().fit(dataset, self.graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 466, in fit\n",
      "    self.fit_phase(dataset, current_phase)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 530, in fit_phase\n",
      "    stats.append(node.op.fit(node.input_columns, Dataset(ddf)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/ops/subgraph.py\", line 75, in fit\n",
      "    DaskExecutor().fit(dataset, self.graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 466, in fit\n",
      "    self.fit_phase(dataset, current_phase)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 520, in fit_phase\n",
      "    transformed_ddf = self.transform(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py\", line 401, in transform\n",
      "    return ensure_optimize_dataframe_graph(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/merlin/core/utils.py\", line 134, in ensure_optimize_dataframe_graph\n",
      "    ddf.dask = dsk\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/dask_expr/_collection.py\", line 3061, in __setattr__\n",
      "    object.__setattr__(self, key, value)\n",
      "AttributeError: can't set attribute 'dask'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransform_aliccp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvt_workflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnvt_wkflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkflow_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworkflow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/datasets/ecommerce/aliccp/dataset.py:267\u001b[0m, in \u001b[0;36mtransform_aliccp\u001b[0;34m(data, output_path, nvt_workflow, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata must be a path or a tuple of train and valid datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m \u001b[43mworkflow_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnvt_workflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/models/utils/example_utils.py:33\u001b[0m, in \u001b[0;36mworkflow_fit_transform\u001b[0;34m(outputs, train, valid, output_path, workflow_name, save_workflow)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     31\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(valid_path)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m workflow\u001b[38;5;241m.\u001b[39mtransform(_train)\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;28mstr\u001b[39m(train_path))\n\u001b[1;32m     35\u001b[0m workflow\u001b[38;5;241m.\u001b[39mtransform(_valid)\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;28mstr\u001b[39m(valid_path))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nvtabular/workflow/workflow.py:213\u001b[0m, in \u001b[0;36mWorkflow.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: Dataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates statistics for this workflow on the input dataset\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        This Workflow with statistics calculated on it\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:466\u001b[0m, in \u001b[0;36mDaskExecutor.fit\u001b[0;34m(self, dataset, graph, refit)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_phase:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# this shouldn't happen, but lets not infinite loop just in case\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to find dependency-free StatOperator to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_phase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Remove all the operators we processed in this phase, and remove\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# from the dependencies of other ops too\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m current_phase:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:530\u001b[0m, in \u001b[0;36mDaskExecutor.fit_phase\u001b[0;34m(self, dataset, nodes, strict)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mis_subgraph:\n\u001b[0;32m--> 530\u001b[0m         stats\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m         stats\u001b[38;5;241m.\u001b[39mappend(node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mfit(node\u001b[38;5;241m.\u001b[39minput_columns, transformed_ddf))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/ops/subgraph.py:75\u001b[0m, in \u001b[0;36mSubgraph.fit\u001b[0;34m(self, col_selector, dataset)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m, col_selector: ColumnSelector, dataset: Dataset\n\u001b[1;32m     74\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=arguments-renamed\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mDaskExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:466\u001b[0m, in \u001b[0;36mDaskExecutor.fit\u001b[0;34m(self, dataset, graph, refit)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_phase:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# this shouldn't happen, but lets not infinite loop just in case\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to find dependency-free StatOperator to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_phase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Remove all the operators we processed in this phase, and remove\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# from the dependencies of other ops too\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m current_phase:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:530\u001b[0m, in \u001b[0;36mDaskExecutor.fit_phase\u001b[0;34m(self, dataset, nodes, strict)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mis_subgraph:\n\u001b[0;32m--> 530\u001b[0m         stats\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m         stats\u001b[38;5;241m.\u001b[39mappend(node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mfit(node\u001b[38;5;241m.\u001b[39minput_columns, transformed_ddf))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/ops/subgraph.py:75\u001b[0m, in \u001b[0;36mSubgraph.fit\u001b[0;34m(self, col_selector, dataset)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m, col_selector: ColumnSelector, dataset: Dataset\n\u001b[1;32m     74\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=arguments-renamed\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mDaskExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:466\u001b[0m, in \u001b[0;36mDaskExecutor.fit\u001b[0;34m(self, dataset, graph, refit)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_phase:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# this shouldn't happen, but lets not infinite loop just in case\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to find dependency-free StatOperator to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_phase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Remove all the operators we processed in this phase, and remove\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# from the dependencies of other ops too\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m current_phase:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:520\u001b[0m, in \u001b[0;36mDaskExecutor.fit_phase\u001b[0;34m(self, dataset, nodes, strict)\u001b[0m\n\u001b[1;32m    514\u001b[0m     addl_input_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(node\u001b[38;5;241m.\u001b[39minput_columns\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    515\u001b[0m         upstream_output_cols\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# apply transforms necessary for the inputs to the current column group, ignoring\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# the transforms from the statop itself\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m transformed_ddf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparents_with_dependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddl_input_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mis_subgraph:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dag/executors.py:401\u001b[0m, in \u001b[0;36mDaskExecutor.transform\u001b[0;34m(self, ddf, graph, output_dtypes, additional_columns, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_dtypes:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# TODO: constructing meta like this loses dtype information on the ddf\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# and sets it all to 'float64'. We should propagate dtype information along\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# with column names in the columngroup graph. This currently only\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# happens during intermediate 'fit' transforms, so as long as statoperators\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# don't require dtype information on the DDF this doesn't matter all that much\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     output_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(ddf\u001b[38;5;241m.\u001b[39m_meta)({k: [] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns})\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensure_optimize_dataframe_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/core/utils.py:134\u001b[0m, in \u001b[0;36mensure_optimize_dataframe_graph\u001b[0;34m(ddf, dsk, keys)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dsk\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Return optimized ddf\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdask\u001b[49m \u001b[38;5;241m=\u001b[39m dsk\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ddf\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask_expr/_collection.py:3061\u001b[0m, in \u001b[0;36mDataFrame.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3059\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3061\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute 'dask'"
     ]
    }
   ],
   "source": [
    "transform_aliccp(\n",
    "    (train_raw, valid_raw), output_path, nvt_workflow=nvt_wkflow, workflow_name=\"workflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b454836-2caf-4d8f-85b4-a912bfabb92a",
   "metadata": {},
   "source": [
    "## Training a Retrieval Model with Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70eedd-021d-4ac5-99b8-e4cd1c9b8689",
   "metadata": {},
   "source": [
    "We start with the offline candidate retrieval stage. We are going to train a Two-Tower model for item retrieval. To learn more about the Two-tower model you can visit [05-Retrieval-Model.ipynb](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b90b28-c71a-4477-a996-7fd0204ad5ef",
   "metadata": {},
   "source": [
    "We are going to process our raw categorical features by encoding them using Categorify() operator and tag the features with user or item tags in the schema file. To learn more about NVTabular and the schema object visit this example notebook in the Merlin Models repo. \\\n",
    "Define a new output path to store the filtered datasets and schema files.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/NVTabular \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4cf2fd7-67d8-473b-a349-cc9a38ef8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path2 = os.path.join(DATA_FOLDER, \"processed/retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0eb5a0f7-e02a-4b41-b664-38e627416f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_tt \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m valid_tt \u001b[38;5;241m=\u001b[39m Dataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:319\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, path_or_source, engine, npartitions, part_size, part_mem_fraction, storage_options, dtypes, client, cpu, base_dataset, schema, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(engine, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDatasetEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m CSVDatasetEngine(\n\u001b[1;32m    324\u001b[0m             paths, part_size, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/io/parquet.py:335\u001b[0m, in \u001b[0;36mParquetDatasetEngine.__init__\u001b[0;34m(self, paths, part_size, storage_options, row_groups_per_part, legacy, batch_size, cpu, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_parquet_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_groups_per_part \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_real_meta, rg_byte_size_0 \u001b[38;5;241m=\u001b[39m run_on_worker(\n\u001b[1;32m    334\u001b[0m         _sample_row_group,\n\u001b[0;32m--> 335\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path0\u001b[49m,\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs,\n\u001b[1;32m    337\u001b[0m         cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu,\n\u001b[1;32m    338\u001b[0m         memory_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_parquet_kwargs,\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m     row_groups_per_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart_size \u001b[38;5;241m/\u001b[39m rg_byte_size_0\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row_groups_per_part \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/io/parquet.py:360\u001b[0m, in \u001b[0;36mParquetDatasetEngine._path0\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;129m@property\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_path0\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mget_fragments())\u001b[38;5;241m.\u001b[39mpath\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/io/parquet.py:390\u001b[0m, in \u001b[0;36mParquetDatasetEngine._dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pa_ds\u001b[38;5;241m.\u001b[39mdataset(paths, filesystem\u001b[38;5;241m=\u001b[39mfs)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# This is a directory or a single file\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pa_ds\u001b[38;5;241m.\u001b[39mdataset(\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, filesystem\u001b[38;5;241m=\u001b[39mfs)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"))\n",
    "valid_tt = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9dc47b4-d99c-4809-a012-27abe5c201d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = train_tt.schema.column_names\n",
    "outputs = inputs >> Filter(f=lambda df: df[\"click\"] == 1)\n",
    "\n",
    "nvt_wkflow.fit(train_tt)\n",
    "\n",
    "nvt_wkflow.transform(train_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"train\")\n",
    ")\n",
    "\n",
    "nvt_wkflow.transform(valid_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6c95b-3b24-463b-b587-147b9c019f76",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file, schema.pbtxt a protobuf text file, of our processed dataset. To learn more about the schema object and schema file you can explore 02-Merlin-Models-and-NVTabular-integration.ipynb notebook.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8187a40-30a9-4bd9-9e3a-b9c595b3f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path2, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid_tt = Dataset(os.path.join(output_path2, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa62a9aa-e47a-404a-bb66-01f0f477474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train_tt.schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER]).without(['click'])\n",
    "train_tt.schema = schema\n",
    "valid_tt.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c4b6bec-47d3-43c9-92a7-d0f7b6e31bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tt = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b580d34-525f-4064-bcda-5684e2d2d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2024-09-21 10:43:50.067645: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - ETA: 0s - loss: 8.9539 - recall_at_10: 0.0130 - ndcg_at_10: 0.0078 - regularization_loss: 0.0000e+00 - loss_batch: 8.9251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:44:13.356851: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 30s 2s/step - loss: 8.9539 - recall_at_10: 0.0132 - ndcg_at_10: 0.0080 - regularization_loss: 0.0000e+00 - loss_batch: 8.8711 - val_loss: 8.9179 - val_recall_at_10: 0.0212 - val_ndcg_at_10: 0.0164 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 8.5806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89ed93a350>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tt.compile(\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=False,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)],\n",
    ")\n",
    "model_tt.fit(train_tt, validation_data=valid_tt, batch_size=1024 * 8, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "743d9d98-81e8-4d98-bb5d-c26816972c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tower = model_tt.retrieval_block.query_block()\n",
    "query_tower.save(os.path.join(BASE_DIR, \"query_tower\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934917ea-d24f-49c9-9e9e-ebcaec9d95dd",
   "metadata": {},
   "source": [
    "## Training a Ranking Model with DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f425d1b-3d5a-44c3-bb16-da3415506dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a40bcf0-8177-4e67-96f8-4351302a0690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000b4f3-3695-4fe6-8b85-c438809fc342",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model (DLRM) architecture is a popular neural network model originally proposed by Facebook in 2019. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in here. To learn more about DLRM architetcture please visit `Exploring-different-models` notebook in the Merlin Models GH repo.\n",
    "\n",
    "https://arxiv.org/abs/1906.00091 \\\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5694074 \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/04-Exporting-ranking-models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0609d305-9028-46e9-a9cd-eaa5852497aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c873dabd-1b6c-48b9-bef7-5a3521445344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:47:54.879876: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.6932 - auc: 0.5000 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:47:59.860910: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 6s 370ms/step - loss: 0.6932 - auc: 0.5000 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932 - val_loss: 0.6932 - val_auc: 0.4983 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89a5a5db40>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=16 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "187c8421-2efd-4ed4-87cf-c069b56c6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(BASE_DIR, \"dlrm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80223f-ac70-44f9-8cd6-59f859a78a6d",
   "metadata": {},
   "source": [
    "In the following cells we are going to export the required user and item features files, and save the query (user) tower model and item embeddings to disk. If you want to read more about exporting retrieval models, please visit 05-Retrieval-Model.ipynb notebook in Merlin Models library repo.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81b388-8d64-437f-836d-7b6c6d2deb2c",
   "metadata": {},
   "source": [
    "## Extract and save Item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "655af406-0af4-40d2-8e04-cc7ba6475ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "workflow =  nvt.Workflow([\"item_id\"] + (['item_id', 'item_brand', 'item_category', 'item_shop'] >> TransformWorkflow(nvt_wkflow.get_subworkflow(\"item\")) >> PredictTensorflow(model_tt.first.item_block())))\n",
    "item_embeddings = workflow.fit_transform(Dataset(item_features)).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c889750-5ac0-4e22-918f-d1e1c64d2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     item_id                                           output_1\n",
      "453      309  [0.029576169326901436, 0.006714393850415945, -...\n",
      "454      591  [0.00835354346781969, -0.0035651458892971277, ...\n",
      "455      329  [0.025229154154658318, 0.004694434814155102, -...\n",
      "456      401  [0.01725717820227146, -0.00310110324062407, -0...\n",
      "457      641  [0.00835354346781969, -0.0035651458892971277, ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(item_embeddings.tail())\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bbbf872-4668-4ba9-919f-5500bc88e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embeddings.to_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88c7a-c955-45a8-aa87-ad29b7b4012f",
   "metadata": {},
   "source": [
    "## Create feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0a0c007-7f01-4be6-9b3d-bca277fe95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /try-merlin/feast_repo/user_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/feature_repo/user_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "user_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/feature_repo/data/user_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "user = Entity(name=\"user_id\", value_type=ValueType.INT32, join_keys=[\"user_id\"],)\n",
    "\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[user],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"user_shops\", dtype=Int32),\n",
    "        Field(name=\"user_profile\", dtype=Int32),\n",
    "        Field(name=\"user_group\", dtype=Int32),\n",
    "        Field(name=\"user_gender\", dtype=Int32),\n",
    "        Field(name=\"user_age\", dtype=Int32),\n",
    "        Field(name=\"user_consumption_2\", dtype=Int32),\n",
    "        Field(name=\"user_is_occupied\", dtype=Int32),\n",
    "        Field(name=\"user_geography\", dtype=Int32),\n",
    "        Field(name=\"user_intentions\", dtype=Int32),\n",
    "        Field(name=\"user_brands\", dtype=Int32),\n",
    "        Field(name=\"user_categories\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=user_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6afb5241-346f-413a-825d-c76b4c888f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /try-merlin/feast_repo/item_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/feature_repo/item_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/feature_repo/data/item_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"item_id\", value_type=ValueType.INT32, join_keys=[\"item_id\"],)\n",
    "\n",
    "item_features_view = FeatureView(\n",
    "    name=\"item_features\",\n",
    "    entities=[item],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"item_category\", dtype=Int32),\n",
    "        Field(name=\"item_shop\", dtype=Int32),\n",
    "        Field(name=\"item_brand\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=item_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e5355e9-7464-42ed-9069-f9935f5c3340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seedir\n",
      "  Downloading seedir-0.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting natsort (from seedir)\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading seedir-0.5.0-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort, seedir\n",
      "Successfully installed natsort-8.4.0 seedir-0.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install seedir\n",
    "!pip install seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23663ade-7a0d-450b-924b-285a34a8486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feast_repo/\n",
      "├─README.md\n",
      "├─__init__.py\n",
      "├─feature_repo/\n",
      "│ ├─__init__.py\n",
      "│ ├─__pycache__/\n",
      "│ │ ├─__init__.cpython-310.pyc\n",
      "│ │ ├─example_repo.cpython-310.pyc\n",
      "│ │ └─test_workflow.cpython-310.pyc\n",
      "│ ├─data/\n",
      "│ │ ├─item_features.parquet\n",
      "│ │ └─user_features.parquet\n",
      "│ ├─feature_store.yaml\n",
      "│ └─test_workflow.py\n",
      "├─item_features.py\n",
      "└─user_features.py\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo\")\n",
    "sd.seedir(\n",
    "    feature_repo_path,\n",
    "    style=\"lines\",\n",
    "    itemlimit=10,\n",
    "    depthlimit=3,\n",
    "    exclude_folders=\".ipynb_checkpoints\",\n",
    "    sort=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cf2d0-ac85-49ef-a022-e09e6dc76b91",
   "metadata": {},
   "source": [
    "We trained and exported our ranking and retrieval models and NVTabular workflows. In the next step, we will learn how to deploy our trained models into Triton Inference Server (TIS) with Merlin Systems library.\n",
    "\n",
    "For the next step, move on to the 02-Deploying-multi-stage-Recsys-with-Merlin-Systems.ipynb notebook to deploy our saved models as an ensemble to TIS and obtain prediction results for a given reques\n",
    "\n",
    "https://github.com/triton-inference-server/servert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2ef13-95c8-491c-8645-b8f41bf6af0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
