{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6c151-f2ff-41e4-8f74-6dfd780fe65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow fastparquet pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66324df1-af67-4fa0-99fe-fc271f7b8d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 09:53:45.922172: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-21 09:53:46.166917: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Rename, Filter, Dropna, LambdaOp, Categorify, \\\n",
    "    TagAsUserFeatures, TagAsUserID, TagAsItemFeatures, TagAsItemID, AddMetadata\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.dag.ops.subgraph import Subgraph\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.datasets.ecommerce import transform_aliccp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ff73ad-2ab1-4e37-bd9e-12dc74d204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/try-merlin/data/\")\n",
    "# set up the base dir for feature store\n",
    "BASE_DIR = os.environ.get(\n",
    "    \"BASE_DIR\", \"/try-merlin/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83a7655-af88-4970-8877-258e581b5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.datasets.synthetic import generate_data\n",
    "\n",
    "NUM_ROWS = os.environ.get(\"NUM_ROWS\", 100_000)\n",
    "train_raw, valid_raw = generate_data(\"aliccp-raw\", int(NUM_ROWS), set_sizes=(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ffb79-fb46-486f-a5ad-e1fddb74f72e",
   "metadata": {},
   "source": [
    "If you would like to use the real ALI-CCP dataset, you can use [get_aliccp()](https://github.com/NVIDIA-Merlin/models/blob/stable/merlin/datasets/ecommerce/aliccp/dataset.py) function instead. This function takes the raw csv files, and generate parquet files that can be directly fed to NVTabular workflow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec19d36-c707-454c-a02e-9e874a578122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac06fd0-dd35-4c34-a8be-f53938502234",
   "metadata": {},
   "source": [
    "## Set up a feature store with Feast\r\n",
    "Before we move onto the next step, we need to create a Feast feature repository.[Feast](https://feast.dev/)t is an end-to-end open source feature store for machine learning. Feast (Feature Store) is a customizable operational data system that re-uses existing infrastructure to manage and serve machine learning features to real-time models.\r\n",
    "\r\n",
    "We will create the feature repo in the current working directory, which `BASE_DIR`DIR for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10a190d-62b8-4235-9bee-6a7cb1a595ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feast\n",
      "  Downloading feast-0.40.1-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from feast) (8.1.7)\n",
      "Collecting colorama<1,>=0.3.9 (from feast)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting dill~=0.3.0 (from feast)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mypy-protobuf>=3.1 (from feast)\n",
      "  Downloading mypy_protobuf-3.6.0-py3-none-any.whl.metadata (466 bytes)\n",
      "Requirement already satisfied: Jinja2<4,>=2 in /usr/local/lib/python3.10/dist-packages (from feast) (3.1.3)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from feast) (4.20.0)\n",
      "Collecting mmh3 (from feast)\n",
      "  Downloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22 in /usr/local/lib/python3.10/dist-packages (from feast) (1.22.4)\n",
      "Requirement already satisfied: pandas<3,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from feast) (1.5.2)\n",
      "Collecting protobuf<5.0.0,>=4.24.0 (from feast)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pyarrow>=4 in /usr/local/lib/python3.10/dist-packages (from feast) (10.0.1.dev0+ga6eabc2b.d20230428)\n",
      "Collecting pydantic>=2.0.0 (from feast)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m332.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from feast) (2.17.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from feast) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from feast) (2.31.0)\n",
      "Collecting SQLAlchemy>1 (from SQLAlchemy[mypy]>1->feast)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting tabulate<1,>=0.8.0 (from feast)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tenacity<9,>=7 (from feast)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<1,>=0.10.0 (from feast)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4 in /usr/local/lib/python3.10/dist-packages (from feast) (4.66.1)\n",
      "Collecting typeguard>=4.0.0 (from feast)\n",
      "  Downloading typeguard-4.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting fastapi>=0.68.0 (from feast)\n",
      "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn<1,>=0.14.0 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting dask>=2024.2.1 (from dask[dataframe]>=2024.2.1->feast)\n",
      "  Downloading dask-2024.9.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting gunicorn (from feast)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (2022.5.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (1.4.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (0.12.0)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting pandas<3,>=1.4.3 (from feast)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m87.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2024.2.1->feast)\n",
      "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.68.0->feast)\n",
      "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.68.0->feast) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2->feast) (2.1.3)\n",
      "Collecting types-protobuf>=4.24 (from mypy-protobuf>=3.1->feast)\n",
      "  Downloading types_protobuf-5.27.0.20240920-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.3->feast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.3->feast) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.3->feast)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->feast)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=2.0.0->feast)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>1->SQLAlchemy[mypy]>1->feast) (3.0.3)\n",
      "Collecting mypy>=0.910 (from SQLAlchemy[mypy]>1->feast)\n",
      "  Downloading mypy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from fastapi>=0.68.0->feast)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting h11>=0.8 (from uvicorn<1,>=0.14.0->uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]<1,>=0.14.0->feast)\n",
      "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->feast) (0.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->feast) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->feast) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->feast) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->feast) (2023.11.17)\n",
      "Collecting pyarrow>=4 (from feast)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=4.13.0->dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]>1->feast) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy>=0.910->SQLAlchemy[mypy]>1->feast) (2.0.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.3->feast) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi>=0.68.0->feast) (4.2.0)\n",
      "Collecting numpy<2,>=1.22 (from feast)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m144.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi>=0.68.0->feast) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi>=0.68.0->feast) (1.2.0)\n",
      "Downloading feast-0.40.1-py2.py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m168.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading dask-2024.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mypy_protobuf-3.6.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading typeguard-4.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading dask_expr-1.1.14-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading mypy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_protobuf-5.27.0.20240920-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: zipp, websockets, uvloop, tzdata, typing-extensions, types-protobuf, toml, tenacity, tabulate, python-dotenv, protobuf, numpy, mmh3, httptools, h11, gunicorn, dill, colorama, annotated-types, uvicorn, typeguard, SQLAlchemy, pydantic-core, pyarrow, pandas, mypy-protobuf, mypy, importlib-metadata, faiss-cpu, watchfiles, starlette, pydantic, dask, fastapi, dask-expr, feast\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 1.0.0\n",
      "    Uninstalling zipp-1.0.0:\n",
      "      Successfully uninstalled zipp-1.0.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 10.0.1.dev0+ga6eabc2b.d20230428\n",
      "    Uninstalling pyarrow-10.0.1.dev0+ga6eabc2b.d20230428:\n",
      "      Successfully uninstalled pyarrow-10.0.1.dev0+ga6eabc2b.d20230428\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.2\n",
      "    Uninstalling pandas-1.5.2:\n",
      "      Successfully uninstalled pandas-1.5.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Uninstalling importlib-metadata-4.6.4:\n",
      "      Successfully uninstalled importlib-metadata-4.6.4\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2023.1.1\n",
      "    Uninstalling dask-2023.1.1:\n",
      "      Successfully uninstalled dask-2023.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.4.0 requires cupy-cuda12x, which is not installed.\n",
      "dask-cudf 23.4.0 requires cupy-cuda12x, which is not installed.\n",
      "transformers4rec 23.12.0 requires torchmetrics>=0.10.0, which is not installed.\n",
      "cudf 23.4.0 requires cuda-python>=12, but you have cuda-python 11.8.3 which is incompatible.\n",
      "cudf 23.4.0 requires numba<0.57,>=0.56.4, but you have numba 0.58.1 which is incompatible.\n",
      "cudf 23.4.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.3 which is incompatible.\n",
      "cudf 23.4.0 requires protobuf==3.20.3, but you have protobuf 4.25.5 which is incompatible.\n",
      "cupy 12.0.0b3 requires numpy<1.26,>=1.20, but you have numpy 1.26.4 which is incompatible.\n",
      "dask-cuda 23.4.0 requires dask==2023.3.2, but you have dask 2024.9.0 which is incompatible.\n",
      "dask-cuda 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2023.1.1 which is incompatible.\n",
      "dask-cuda 23.4.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.3 which is incompatible.\n",
      "dask-cudf 23.4.0 requires dask==2023.3.2, but you have dask 2024.9.0 which is incompatible.\n",
      "dask-cudf 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2023.1.1 which is incompatible.\n",
      "dask-cudf 23.4.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.3 which is incompatible.\n",
      "distributed 2023.1.1 requires dask==2023.1.1, but you have dask 2024.9.0 which is incompatible.\n",
      "merlin-core 23.8.0 requires fsspec>=2022.7.1, but you have fsspec 2022.5.0 which is incompatible.\n",
      "merlin-core 23.8.0 requires pandas<1.6.0dev0,>=1.2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "rmm 23.4.0 requires cuda-python>=12, but you have cuda-python 11.8.3 which is incompatible.\n",
      "tensorflow 2.12.0+nv23.6 requires numpy<1.24,>=1.22; python_version >= \"3.7\", but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.5 which is incompatible.\n",
      "tf2onnx 1.16.0 requires protobuf~=3.20.2, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 annotated-types-0.7.0 colorama-0.4.6 dask-2024.9.0 dask-expr-1.1.14 dill-0.3.8 faiss-cpu-1.8.0.post1 fastapi-0.115.0 feast-0.40.1 gunicorn-23.0.0 h11-0.14.0 httptools-0.6.1 importlib-metadata-8.5.0 mmh3-5.0.0 mypy-1.11.2 mypy-protobuf-3.6.0 numpy-1.26.4 pandas-2.2.3 protobuf-4.25.5 pyarrow-17.0.0 pydantic-2.9.2 pydantic-core-2.23.4 python-dotenv-1.0.1 starlette-0.38.5 tabulate-0.9.0 tenacity-8.5.0 toml-0.10.2 typeguard-4.3.0 types-protobuf-5.27.0.20240920 typing-extensions-4.12.2 tzdata-2024.1 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1 zipp-3.20.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/try-merlin/feast_repo\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install feast faiss-cpu\n",
    "!rm -rf $BASE_DIR/feast_repo\n",
    "!cd $BASE_DIR && feast init feast_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1d39db-6610-4d24-a5ab-580b84892efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo/feature_repo\")\n",
    "if os.path.exists(f\"{feature_repo_path}/example_repo.py\"):\n",
    "    os.remove(f\"{feature_repo_path}/example_repo.py\")\n",
    "if os.path.exists(f\"{feature_repo_path}/data/driver_stats.parquet\"):\n",
    "    os.remove(f\"{feature_repo_path}/data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20807ba-cbb0-426b-8c71-f854cacb7e4f",
   "metadata": {},
   "source": [
    "## Exporting user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebda30a7-961c-4a4c-8c4d-9bfec7598f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "user_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.USER, Tags.USER_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27805d65-6fc8-43d6-af10-18c1beac7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "user_features[\"datetime\"] = datetime.now()\n",
    "user_features[\"datetime\"] = user_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "user_features[\"created\"] = datetime.now()\n",
    "user_features[\"created\"] = user_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1bbe23-35fe-42bd-8cbf-d2586bbc6a53",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:1105\u001b[0m, in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1103\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m dtype \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# TODO(EA2D) special case would be unnecessary with 2D EAs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1110\u001b[0m, in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_array\u001b[39m(\n\u001b[1;32m   1080\u001b[0m     values: ArrayLike,\n\u001b[1;32m   1081\u001b[0m     formatter: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     fallback_formatter: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1091\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    Format an array for printing.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    values : np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m    formatter\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    float_format\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    na_rep\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m    digits\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m    space\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m    justify\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m    decimal\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    leading_space : bool, optional, default True\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m        Whether the array should be formatted with a leading space.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m        When an array as a column of a Series or DataFrame, we do want\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m        the leading space to pad between columns.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \n\u001b[0;32m-> 1110\u001b[0m \u001b[38;5;124;03m        When formatting an Index subclass\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m        (e.g. IntervalIndex._get_values_for_csv), we don't want the\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m        leading space since it should be left-aligned.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    fallback_formatter\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m    List[str]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     fmt_klass: \u001b[38;5;28mtype\u001b[39m[_GenericArrayFormatter]\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(values\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:88\u001b[0m, in \u001b[0;36mHTMLFormatter.to_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines):\n\u001b[1;32m     90\u001b[0m         lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:644\u001b[0m, in \u001b[0;36mNotebookFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_style()\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:94\u001b[0m, in \u001b[0;36mHTMLFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[1;32m     97\u001b[0m         by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m215\u001b[39m)  \u001b[38;5;66;03m# ×  # noqa: RUF003\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:267\u001b[0m, in \u001b[0;36mHTMLFormatter._write_table\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<table\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborder_attr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    263\u001b[0m     indent,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_body(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</table>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:403\u001b[0m, in \u001b[0;36mHTMLFormatter._write_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<thead>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_col_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_row_header(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:383\u001b[0m, in \u001b[0;36mHTMLFormatter._write_col_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m row\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_columns_formatted_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    384\u001b[0m align \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mjustify\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_truncated_horizontally:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:611\u001b[0m, in \u001b[0;36mNotebookFormatter._get_columns_formatted_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_formatted_values\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# only reached with non-Multi Index\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_flat\u001b[49m(include_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_flat'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    user_id  user_shops  user_profile  user_group  user_gender  user_age  \\\n",
       "24        7         361             1           1            1         1   \n",
       "\n",
       "    user_consumption_1  user_consumption_2  user_is_occupied  user_geography  \\\n",
       "24                   1                   1                 1               1   \n",
       "\n",
       "    user_intentions  user_brands  user_categories                   datetime  \\\n",
       "24              105          180               19 2024-09-21 10:00:06.899731   \n",
       "\n",
       "                      created  \n",
       "24 2024-09-21 10:00:06.901765  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features[user_features[\"user_id\"] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067b1e25-6472-45df-9cae-68dbb3101bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"user_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c06bae-2410-44b7-99d6-4067036d5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "item_features = (\n",
    "    unique_rows_by_features(train_raw, Tags.ITEM, Tags.ITEM_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103877e4-c4ab-4a38-8614-ec630ea92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features[\"datetime\"] = datetime.now()\n",
    "item_features[\"datetime\"] = item_features[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "item_features[\"created\"] = datetime.now()\n",
    "item_features[\"created\"] = item_features[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33e2274-ca14-4034-826b-aeb454663a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:1105\u001b[0m, in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1103\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m dtype \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# TODO(EA2D) special case would be unnecessary with 2D EAs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1110\u001b[0m, in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_array\u001b[39m(\n\u001b[1;32m   1080\u001b[0m     values: ArrayLike,\n\u001b[1;32m   1081\u001b[0m     formatter: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     fallback_formatter: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1091\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    Format an array for printing.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    values : np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m    formatter\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    float_format\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    na_rep\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m    digits\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m    space\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m    justify\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m    decimal\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    leading_space : bool, optional, default True\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m        Whether the array should be formatted with a leading space.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m        When an array as a column of a Series or DataFrame, we do want\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m        the leading space to pad between columns.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \n\u001b[0;32m-> 1110\u001b[0m \u001b[38;5;124;03m        When formatting an Index subclass\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m        (e.g. IntervalIndex._get_values_for_csv), we don't want the\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m        leading space since it should be left-aligned.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    fallback_formatter\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m    List[str]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     fmt_klass: \u001b[38;5;28mtype\u001b[39m[_GenericArrayFormatter]\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(values\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:88\u001b[0m, in \u001b[0;36mHTMLFormatter.to_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines):\n\u001b[1;32m     90\u001b[0m         lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:644\u001b[0m, in \u001b[0;36mNotebookFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_style()\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:94\u001b[0m, in \u001b[0;36mHTMLFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[1;32m     97\u001b[0m         by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m215\u001b[39m)  \u001b[38;5;66;03m# ×  # noqa: RUF003\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:267\u001b[0m, in \u001b[0;36mHTMLFormatter._write_table\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<table\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborder_attr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    263\u001b[0m     indent,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_body(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</table>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:403\u001b[0m, in \u001b[0;36mHTMLFormatter._write_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<thead>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_col_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_row_header(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:383\u001b[0m, in \u001b[0;36mHTMLFormatter._write_col_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m row\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_columns_formatted_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    384\u001b[0m align \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mjustify\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_truncated_horizontally:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/html.py:611\u001b[0m, in \u001b[0;36mNotebookFormatter._get_columns_formatted_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_formatted_values\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# only reached with non-Multi Index\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_flat\u001b[49m(include_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_flat'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   item_id  item_category  item_shop  item_brand  item_intention  \\\n",
       "0       84            247      17392        5990            2770   \n",
       "1       46            134       9429        3248            1502   \n",
       "2       10             27       1886         650             301   \n",
       "3        3              6        420         145              67   \n",
       "4       21             60       4191        1444             668   \n",
       "\n",
       "                    datetime                    created  \n",
       "0 2024-09-21 10:07:33.071318 2024-09-21 10:07:33.073730  \n",
       "1 2024-09-21 10:07:33.071318 2024-09-21 10:07:33.073730  \n",
       "2 2024-09-21 10:07:33.071318 2024-09-21 10:07:33.073730  \n",
       "3 2024-09-21 10:07:33.071318 2024-09-21 10:07:33.073730  \n",
       "4 2024-09-21 10:07:33.071318 2024-09-21 10:07:33.073730  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "588a6794-f801-4d68-aa09-0f0d89e399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(\n",
    "    os.path.join(feature_repo_path, \"data\", \"item_features.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9dd05-fe8d-444a-ba7c-183b15280ff2",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818a5ab4-736a-4712-af0e-7789112d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_FOLDER, \"processed_nvt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e15285-048a-4b23-81a3-b52111c4084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the user_id and item_id to the pipeline: Rename (add postfix \"_raw\") -> Casttype. And Mark these as Feature of Recsys\n",
    "user_id_raw = [\"user_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsUserFeatures()\n",
    "item_id_raw = [\"item_id\"] >> Rename(postfix='_raw') >> LambdaOp(lambda col: col.astype(\"int32\")) >> TagAsItemFeatures()\n",
    "\n",
    "# Feed the item_id, item_category, item_shop, item_brand into the Categorify processing.\n",
    "item_cat = Categorify(dtype=\"int32\")\n",
    "items = ([\"item_id\",\"item_category\", \"item_shop\", \"item_brand\"] >> item_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94c9c02a-2bf6-4e40-bae0-7d36c378c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_item = Subgraph(\n",
    "     \"item\", \n",
    "     Subgraph(\"items_cat\", items) + \n",
    "    (items[\"item_id\"] >> TagAsItemID()) + \n",
    "    (items[\"item_category\", \"item_shop\", \"item_brand\"] >> TagAsItemFeatures())\n",
    ")\n",
    "subgraph_user = Subgraph(\n",
    "    \"user\",\n",
    "    ([\"user_id\"] >> Categorify(dtype=\"int32\") >> TagAsUserID()) +\n",
    "    (\n",
    "        [\n",
    "            \"user_shops\",\n",
    "            \"user_profile\",\n",
    "            \"user_group\",\n",
    "            \"user_gender\",\n",
    "            \"user_age\",\n",
    "            \"user_consumption_2\",\n",
    "            \"user_is_occupied\",\n",
    "            \"user_geography\",\n",
    "            \"user_intentions\",\n",
    "            \"user_brands\",\n",
    "            \"user_categories\",\n",
    "        ] >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fefe559f-f285-47d8-aaec-70ac86609201",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"click\"] >> AddMetadata(tags=[Tags.BINARY_CLASSIFICATION, \"target\"])\n",
    "outputs = subgraph_user + subgraph_item + targets\n",
    "\n",
    "# add dropna op to filter rows with nulls\n",
    "outputs = outputs >> Dropna()\n",
    "nvt_wkflow = nvt.Workflow(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbd354-dfe6-46ac-9adf-250212c5565b",
   "metadata": {},
   "source": [
    "Let’s call transform_aliccp utility function to be able to perform fit and transform steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65801f3a-8cfc-48f1-8908-c10b44b74cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform_aliccp(\n",
    "    (train_raw, valid_raw), output_path, nvt_workflow=nvt_wkflow, workflow_name=\"workflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b454836-2caf-4d8f-85b4-a912bfabb92a",
   "metadata": {},
   "source": [
    "## Training a Retrieval Model with Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70eedd-021d-4ac5-99b8-e4cd1c9b8689",
   "metadata": {},
   "source": [
    "We start with the offline candidate retrieval stage. We are going to train a Two-Tower model for item retrieval. To learn more about the Two-tower model you can visit [05-Retrieval-Model.ipynb](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b90b28-c71a-4477-a996-7fd0204ad5ef",
   "metadata": {},
   "source": [
    "We are going to process our raw categorical features by encoding them using Categorify() operator and tag the features with user or item tags in the schema file. To learn more about NVTabular and the schema object visit this example notebook in the Merlin Models repo. \\\n",
    "Define a new output path to store the filtered datasets and schema files.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/NVTabular \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4cf2fd7-67d8-473b-a349-cc9a38ef8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path2 = os.path.join(DATA_FOLDER, \"processed/retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eb5a0f7-e02a-4b41-b664-38e627416f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"))\n",
    "valid_tt = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9dc47b4-d99c-4809-a012-27abe5c201d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = train_tt.schema.column_names\n",
    "outputs = inputs >> Filter(f=lambda df: df[\"click\"] == 1)\n",
    "\n",
    "nvt_wkflow.fit(train_tt)\n",
    "\n",
    "nvt_wkflow.transform(train_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"train\")\n",
    ")\n",
    "\n",
    "nvt_wkflow.transform(valid_tt).to_parquet(\n",
    "    output_path=os.path.join(output_path2, \"valid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6c95b-3b24-463b-b587-147b9c019f76",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file, schema.pbtxt a protobuf text file, of our processed dataset. To learn more about the schema object and schema file you can explore 02-Merlin-Models-and-NVTabular-integration.ipynb notebook.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8187a40-30a9-4bd9-9e3a-b9c595b3f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tt = Dataset(os.path.join(output_path2, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid_tt = Dataset(os.path.join(output_path2, \"valid\", \"*.parquet\"), part_size=\"500MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa62a9aa-e47a-404a-bb66-01f0f477474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = train_tt.schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER]).without(['click'])\n",
    "train_tt.schema = schema\n",
    "valid_tt.schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c4b6bec-47d3-43c9-92a7-d0f7b6e31bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tt = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b580d34-525f-4064-bcda-5684e2d2d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2024-09-21 10:43:50.067645: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - ETA: 0s - loss: 8.9539 - recall_at_10: 0.0130 - ndcg_at_10: 0.0078 - regularization_loss: 0.0000e+00 - loss_batch: 8.9251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:44:13.356851: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 30s 2s/step - loss: 8.9539 - recall_at_10: 0.0132 - ndcg_at_10: 0.0080 - regularization_loss: 0.0000e+00 - loss_batch: 8.8711 - val_loss: 8.9179 - val_recall_at_10: 0.0212 - val_ndcg_at_10: 0.0164 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 8.5806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89ed93a350>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tt.compile(\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=False,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)],\n",
    ")\n",
    "model_tt.fit(train_tt, validation_data=valid_tt, batch_size=1024 * 8, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "743d9d98-81e8-4d98-bb5d-c26816972c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tower = model_tt.retrieval_block.query_block()\n",
    "query_tower.save(os.path.join(BASE_DIR, \"query_tower\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934917ea-d24f-49c9-9e9e-ebcaec9d95dd",
   "metadata": {},
   "source": [
    "## Training a Ranking Model with DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f425d1b-3d5a-44c3-bb16-da3415506dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a40bcf0-8177-4e67-96f8-4351302a0690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000b4f3-3695-4fe6-8b85-c438809fc342",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model (DLRM) architecture is a popular neural network model originally proposed by Facebook in 2019. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in here. To learn more about DLRM architetcture please visit `Exploring-different-models` notebook in the Merlin Models GH repo.\n",
    "\n",
    "https://arxiv.org/abs/1906.00091 \\\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5694074 \\\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/04-Exporting-ranking-models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0609d305-9028-46e9-a9cd-eaa5852497aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c873dabd-1b6c-48b9-bef7-5a3521445344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:47:54.879876: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.6932 - auc: 0.5000 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:47:59.860910: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 6s 370ms/step - loss: 0.6932 - auc: 0.5000 - regularization_loss: 0.0000e+00 - loss_batch: 0.6932 - val_loss: 0.6932 - val_auc: 0.4983 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89a5a5db40>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit(train, validation_data=valid, batch_size=16 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "187c8421-2efd-4ed4-87cf-c069b56c6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(BASE_DIR, \"dlrm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80223f-ac70-44f9-8cd6-59f859a78a6d",
   "metadata": {},
   "source": [
    "In the following cells we are going to export the required user and item features files, and save the query (user) tower model and item embeddings to disk. If you want to read more about exporting retrieval models, please visit 05-Retrieval-Model.ipynb notebook in Merlin Models library repo.\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/models/blob/stable/examples/05-Retrieval-Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81b388-8d64-437f-836d-7b6c6d2deb2c",
   "metadata": {},
   "source": [
    "## Extract and save Item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "655af406-0af4-40d2-8e04-cc7ba6475ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "workflow =  nvt.Workflow([\"item_id\"] + (['item_id', 'item_brand', 'item_category', 'item_shop'] >> TransformWorkflow(nvt_wkflow.get_subworkflow(\"item\")) >> PredictTensorflow(model_tt.first.item_block())))\n",
    "item_embeddings = workflow.fit_transform(Dataset(item_features)).to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c889750-5ac0-4e22-918f-d1e1c64d2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     item_id                                           output_1\n",
      "453      309  [0.029576169326901436, 0.006714393850415945, -...\n",
      "454      591  [0.00835354346781969, -0.0035651458892971277, ...\n",
      "455      329  [0.025229154154658318, 0.004694434814155102, -...\n",
      "456      401  [0.01725717820227146, -0.00310110324062407, -0...\n",
      "457      641  [0.00835354346781969, -0.0035651458892971277, ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(item_embeddings.tail())\n",
    "except AttributeError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bbbf872-4668-4ba9-919f-5500bc88e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embeddings.to_parquet(os.path.join(BASE_DIR, \"item_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a88c7a-c955-45a8-aa87-ad29b7b4012f",
   "metadata": {},
   "source": [
    "## Create feature definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0a0c007-7f01-4be6-9b3d-bca277fe95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /try-merlin/feast_repo/user_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/user_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "user_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/data/user_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "user = Entity(name=\"user_id\", value_type=ValueType.INT32, join_keys=[\"user_id\"],)\n",
    "\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[user],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"user_shops\", dtype=Int32),\n",
    "        Field(name=\"user_profile\", dtype=Int32),\n",
    "        Field(name=\"user_group\", dtype=Int32),\n",
    "        Field(name=\"user_gender\", dtype=Int32),\n",
    "        Field(name=\"user_age\", dtype=Int32),\n",
    "        Field(name=\"user_consumption_2\", dtype=Int32),\n",
    "        Field(name=\"user_is_occupied\", dtype=Int32),\n",
    "        Field(name=\"user_geography\", dtype=Int32),\n",
    "        Field(name=\"user_intentions\", dtype=Int32),\n",
    "        Field(name=\"user_brands\", dtype=Int32),\n",
    "        Field(name=\"user_categories\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=user_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6afb5241-346f-413a-825d-c76b4c888f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /try-merlin/feast_repo/item_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /try-merlin/feast_repo/item_features.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_features = FileSource(\n",
    "    path=\"/try-merlin/feast_repo/data/item_features.parquet\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"item_id\", value_type=ValueType.INT32, join_keys=[\"item_id\"],)\n",
    "\n",
    "item_features_view = FeatureView(\n",
    "    name=\"item_features\",\n",
    "    entities=[item],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"item_category\", dtype=Int32),\n",
    "        Field(name=\"item_shop\", dtype=Int32),\n",
    "        Field(name=\"item_brand\", dtype=Int32),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=item_features,\n",
    "    tags=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e5355e9-7464-42ed-9069-f9935f5c3340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seedir\n",
      "  Downloading seedir-0.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting natsort (from seedir)\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading seedir-0.5.0-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort, seedir\n",
      "Successfully installed natsort-8.4.0 seedir-0.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install seedir\n",
    "!pip install seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23663ade-7a0d-450b-924b-285a34a8486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feast_repo/\n",
      "├─README.md\n",
      "├─__init__.py\n",
      "├─feature_repo/\n",
      "│ ├─__init__.py\n",
      "│ ├─__pycache__/\n",
      "│ │ ├─__init__.cpython-310.pyc\n",
      "│ │ ├─example_repo.cpython-310.pyc\n",
      "│ │ └─test_workflow.cpython-310.pyc\n",
      "│ ├─data/\n",
      "│ │ ├─item_features.parquet\n",
      "│ │ └─user_features.parquet\n",
      "│ ├─feature_store.yaml\n",
      "│ └─test_workflow.py\n",
      "├─item_features.py\n",
      "└─user_features.py\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "feature_repo_path = os.path.join(BASE_DIR, \"feast_repo\")\n",
    "sd.seedir(\n",
    "    feature_repo_path,\n",
    "    style=\"lines\",\n",
    "    itemlimit=10,\n",
    "    depthlimit=3,\n",
    "    exclude_folders=\".ipynb_checkpoints\",\n",
    "    sort=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cf2d0-ac85-49ef-a022-e09e6dc76b91",
   "metadata": {},
   "source": [
    "We trained and exported our ranking and retrieval models and NVTabular workflows. In the next step, we will learn how to deploy our trained models into Triton Inference Server (TIS) with Merlin Systems library.\r\n",
    "\r\n",
    "For the next step, move on to the 02-Deploying-multi-stage-Recsys-with-Merlin-Systems.ipynb notebook to deploy our saved models as an ensemble to TIS and obtain prediction results for a given reques\n",
    "\n",
    "https://github.com/triton-inference-server/servert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2ef13-95c8-491c-8645-b8f41bf6af0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
